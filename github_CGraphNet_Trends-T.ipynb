{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VULCAN\\workspace\\workspace2\\Bert\\model.py:17: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#!/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import model\n",
    "import random\n",
    "\n",
    "class Args():\n",
    "    is_training = False\n",
    "    layers = 1\n",
    "    rnn_size = 100  # RNN hidden unit num\n",
    "    n_epochs = 1  # number of epochs\n",
    "    batch_size = 50  # default 50\n",
    "    dropout_p_hidden = 1  # dropout probability in hidden layer\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.96  # used for batch norm\n",
    "    decay_steps = 1000\n",
    "    sigma = 0  # used for variable initial\n",
    "    init_as_normal = False\n",
    "    reset_after_session = True  # wheather resetting the hidden state when a session is finished\n",
    "    session_key = 'SessionId'  # column 1\n",
    "    item_key = 'ItemId'  # column 2\n",
    "    time_key = 'Time'  # column 3\n",
    "    grad_cap = 0\n",
    "    test_model = 1  # test model version\n",
    "    base_path = ''\n",
    "    checkpoint_dir = ''  # directory of check point\n",
    "    model_name = ''\n",
    "    loss = 'cross-entropy'\n",
    "    #loss = 'bpr'\n",
    "    final_act = 'softmax'  # activation function of the final layer\n",
    "    hidden_act = 'tanh'  # activation function of the hidden layer\n",
    "    n_items = -1  # number of itemes\n",
    "    ori_file = ''  # ds of data\n",
    "    rnn_type = ''\n",
    "    is_preprocess = 0\n",
    "    bert_layer = 2\n",
    "    head_num = 2\n",
    "    predict_sequence_length = 50\n",
    "    \n",
    "    attn_item_sbj_hidden = 8\n",
    "    \n",
    "    ## keep number of neighbors\n",
    "    max_degree = 100\n",
    "    enrich_seq_num = 2\n",
    "\n",
    "\n",
    "def parseArgs():\n",
    "    parser = argparse.ArgumentParser(description='CGraphNet_Trends args')\n",
    "    parser.add_argument('--layer', default=1, type=int)  # default for single layer GRU\n",
    "    parser.add_argument('--size', default=128, type=int)  # rnn size(dimension)\n",
    "    parser.add_argument('--epoch', default=2, type=int)\n",
    "    parser.add_argument('--batch_size', default=512, type=int)\n",
    "    parser.add_argument('--lr', default=0.001, type=float)  # learning rate\n",
    "    parser.add_argument('--train', default=1, type=int)\n",
    "    parser.add_argument('--test', default=1, type=int)\n",
    "    parser.add_argument('--hidden_act', default='tanh', type=str)\n",
    "    parser.add_argument('--final_act', default='softmax', type=str)\n",
    "    parser.add_argument('--loss', default='cross-entropy', type=str)\n",
    "    #parser.add_argument('--loss', default='bpr', type=str)\n",
    "    parser.add_argument('--dropout', default='0.6', type=float)\n",
    "    parser.add_argument('--test_model', default=0, type=int)\n",
    "    parser.add_argument('--base_path', default='', type=str)\n",
    "    parser.add_argument('--ori_file', default='', type=str)\n",
    "    parser.add_argument('--model_name', default='', type=str)\n",
    "    parser.add_argument('--rnn_type', default='gru', type=str)\n",
    "    parser.add_argument('--decay_steps', default=1000, type=int)\n",
    "    parser.add_argument('--is_preprocess', default=0, type=int)\n",
    "    parser.add_argument('--predict_sequence_length', default=10, type=int)\n",
    "    parser.add_argument('--bert_layer', default=10, type=int)\n",
    "    parser.add_argument('--head_num', default=2, type=int)\n",
    "    parser.add_argument('--attn_item_sbj_hidden', default=8, type=int)\n",
    "    parser.add_argument('--sliding_window', default=3, type=int)\n",
    "    parser.add_argument('--max_degree', default=10, type=int)\n",
    "    parser.add_argument('--enrich_seq_num', default=2, type=int)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['foo']\n",
    "command_line = parseArgs()       # get external parameters\n",
    "args = Args()\n",
    "\n",
    "args.layers = command_line.layer\n",
    "args.rnn_size = command_line.size\n",
    "args.n_epochs = command_line.epoch\n",
    "args.learning_rate = command_line.lr\n",
    "args.is_training = command_line.train\n",
    "args.test_model = command_line.test\n",
    "args.batch_size = command_line.batch_size\n",
    "args.hidden_act = command_line.hidden_act\n",
    "args.final_act = command_line.final_act\n",
    "args.loss = command_line.loss\n",
    "args.test_model = command_line.epoch - 1  # use the model saved by the last epoch\n",
    "args.dropout_p_hidden = 1.0 if args.is_training == 0 else command_line.dropout  # use dropout(0.5) in training phase\n",
    "args.base_path = command_line.base_path\n",
    "args.ori_file = command_line.ori_file\n",
    "args.checkpoint_dir = args.base_path + 'checkpoint_CGraphNet_Trends/' + args.ori_file\n",
    "args.model_name = command_line.model_name\n",
    "args.rnn_type = command_line.rnn_type\n",
    "args.decay_steps = command_line.decay_steps\n",
    "args.is_preprocess = command_line.is_preprocess\n",
    "args.predict_sequence_length = command_line.predict_sequence_length\n",
    "args.bert_layer = command_line.bert_layer\n",
    "args.head_num = command_line.head_num\n",
    "\n",
    "args.sliding_window = command_line.sliding_window\n",
    "\n",
    "args.savedModel = 1\n",
    "\n",
    "args.attn_item_sbj_hidden = command_line.attn_item_sbj_hidden\n",
    "args.max_degree = command_line.max_degree\n",
    "args.enrich_seq_num = command_line.enrich_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.base_path = \"short_text_data/\"\n",
    "args.ori_file = \"Trends-T-new_process\"\n",
    "#2020052812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.checkpoint_dir):\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "        with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('mkdir ' + args.checkpoint_dir)\n",
    "else:\n",
    "    with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "        log_file.write('already exists: ' + args.checkpoint_dir)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"     # specify which GPU(s) to be used\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[int(x)] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['accuracy'] = accuracy_score(Y, Y_)\n",
    "        \n",
    "        # print('Results, using embeddings of dimensionality', len(self.embeddings[X[0]]))\n",
    "        # print('-------------------')\n",
    "        #print(results)\n",
    "        return results\n",
    "        # print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "    \n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import layers\n",
    "import random\n",
    "# from utils import *\n",
    "# from rnn import *\n",
    "import networkx as nx\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class BERT4Rec:\n",
    "    def __init__(self, sess, args):\n",
    "        self.sess = sess\n",
    "        self.is_preprocess = args.is_preprocess\n",
    "\n",
    "        self.is_training = args.is_training\n",
    "        self.layers = args.layers\n",
    "        self.rnn_size = args.rnn_size\n",
    "        self.n_epochs = args.n_epochs\n",
    "\n",
    "        self.dropout_p_hidden = args.dropout_p_hidden\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.decay = args.decay  # default:0.96\n",
    "        self.decay_steps = args.decay_steps  # default:1e4\n",
    "        self.sigma = args.sigma  # param for initializer, default:0\n",
    "        self.init_as_normal = args.init_as_normal  # default:False\n",
    "        self.reset_after_session = args.reset_after_session  # default:True\n",
    "        self.session_key = args.session_key\n",
    "        self.item_key = args.item_key\n",
    "        self.time_key = args.time_key\n",
    "        self.grad_cap = args.grad_cap  # default:0\n",
    "        self.base_path = args.base_path\n",
    "        self.model_name = args.model_name\n",
    "        self.rnn_type = args.rnn_type\n",
    "        self.ori_file = args.ori_file\n",
    "        self.test_model = args.test_model\n",
    "        self.sequence_length = args.predict_sequence_length\n",
    "        self.max_click = 200\n",
    "\n",
    "        self.bert_layer = args.bert_layer\n",
    "        self.head_num = args.head_num\n",
    "        self.um_tag_length = 30\n",
    "        self.um_sbj_length = 30\n",
    "\n",
    "        self.train_path = self.base_path\n",
    "        self.batch_size_pos = args.batch_size\n",
    "        \n",
    "        self.attn_item_sbj_hidden = args.attn_item_sbj_hidden\n",
    "        \n",
    "        \n",
    "        self.max_degree = 100\n",
    "        self.tolerant_time = 40\n",
    "\n",
    "        if self.is_preprocess == 1:\n",
    "            return\n",
    "\n",
    "        self.time_emb_num = 30\n",
    "        self.short_emb_size = 4\n",
    "\n",
    "        if args.hidden_act == 'tanh':\n",
    "            self.hidden_act = self.tanh\n",
    "        elif args.hidden_act == 'relu':\n",
    "            self.hidden_act = self.relu\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # different loss function can try with different final_activation in training phase\n",
    "        if args.loss == 'cross-entropy':  # point-wise ranking loss\n",
    "            if args.final_act == 'tanh':\n",
    "                self.final_activation = self.softmaxth\n",
    "            else:\n",
    "                self.final_activation = self.softmax\n",
    "            self.loss_function = self.cross_entropy\n",
    "        elif args.loss == 'bpr':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activation = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.bpr\n",
    "        elif args.loss == 'top1':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activatin = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.top1\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\"[!] Checkpoint Dir not found\")\n",
    "\n",
    "    def init_model(self):\n",
    "\n",
    "        self.n_items = max(self.ItemIdxList) + 2\n",
    "        #self.n_tags = max(self.TagIdxList) + 2\n",
    "\n",
    "        self.predictTopN = self.n_items\n",
    "        # if max(self.ItemIdxList) > 100000:\n",
    "        #     self.predictTopN = 100000\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        print(\"init1\")\n",
    "        if self.is_training == 1:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        print(\"init2\")\n",
    "        #         self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n",
    "        self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        print(\"init3\")\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # use self.predict_state to hold hidden states during prediction.\n",
    "        #         self.predict_state = [np.zeros([self.batch_size, self.rnn_size], dtype=np.float32) for _ in range(self.layers)]\n",
    "        print(\"init4\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, '{}/bert-model-{}'.format(self.checkpoint_dir, self.test_model))\n",
    "        else:\n",
    "            print('no {}/bert-model-{} !!!!!'.format(self.checkpoint_dir, self.test_model))\n",
    "        print(\"init5\")\n",
    "\n",
    "    ########################ACTIVATION FUNCTIONS#########################\n",
    "    def linear(self, X):\n",
    "        return X\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return tf.nn.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        return tf.nn.softmax(X)\n",
    "\n",
    "    def softmaxth(self, X):\n",
    "        return tf.nn.softmax(tf.tanh(X))\n",
    "\n",
    "    def relu(self, X):\n",
    "        return tf.nn.relu(X)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return tf.nn.sigmoid(X)\n",
    "\n",
    "    ############################LOSS FUNCTIONS######################\n",
    "    def cross_entropy(self, yhat):  # yhat is a tensor with shape of [batch_size, batch_size]\n",
    "        return tf.reduce_mean(-tf.log(tf.diag_part(yhat) + 1e-24))\n",
    "\n",
    "    def corss_entropy_2(self, yhat):\n",
    "        return tf.reduce_mean(-tf.log(yhat[:, 0] + 1e-24))\n",
    "\n",
    "    def bpr_2(self, yhat):\n",
    "        yhat_pos = yhat[:, 0]\n",
    "        yhat_pos_column = tf.reshape(yhat_pos, [-1, 1])\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(yhat_pos_column - yhat)))\n",
    "\n",
    "    def bpr(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(tf.diag_part(yhat) - yhatT)))\n",
    "\n",
    "    def top1(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat) + yhatT) + tf.nn.sigmoid(yhatT ** 2), axis=0)\n",
    "        term2 = tf.nn.sigmoid(tf.diag_part(yhat) ** 2) / self.batch_size\n",
    "        return tf.reduce_mean(term1 - term2)\n",
    "\n",
    "    def build_transformer(self, layer_num, head_num, model_size, initializer):\n",
    "        W_TRFM = {}\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                # define qkv\n",
    "                variable_WQ = 'WQ_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WQ] = tf.get_variable(variable_WQ, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "                variable_WK = 'WK_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WK] = tf.get_variable(variable_WK, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "                variable_WV = 'WV_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WV] = tf.get_variable(variable_WV, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "            # define ffn variables\n",
    "            variable_WFFN = 'WFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_WFFN] = tf.get_variable(variable_WFFN, [model_size, model_size], initializer=initializer)\n",
    "\n",
    "            variable_BFFN = 'BFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_BFFN] = tf.get_variable(variable_BFFN, [1, 1, model_size], initializer=initializer)\n",
    "\n",
    "        #             # define ffn variables\n",
    "        #             variable_WFFN = 'WFFN2_layer_' + str(layer)\n",
    "        #             W_TRFM[variable_WFFN] = tf.get_variable(variable_WFFN, [model_size, model_size], initializer=initializer)\n",
    "\n",
    "        #             variable_BFFN = 'BFFN2_layer_' + str(layer)\n",
    "        #             W_TRFM[variable_BFFN] = tf.get_variable(variable_BFFN, [1, 1, model_size], initializer=initializer)\n",
    "\n",
    "        return W_TRFM\n",
    "\n",
    "    def run_transformer(self, layer_num, head_num, model_size, input_trfm, W_TRFM):\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                attn_Q = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WQ_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_K = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WK_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_V = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WV_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_out = tf.matmul(tf.nn.softmax(\n",
    "                    tf.matmul(attn_Q, tf.transpose(attn_K, perm=[0, 2, 1])) / tf.sqrt(tf.cast(model_size, tf.float32))),\n",
    "                    attn_V)\n",
    "                if head == 0:\n",
    "                    attn_concat = attn_out\n",
    "                else:\n",
    "                    attn_concat = tf.concat([attn_concat, attn_out], 2)\n",
    "\n",
    "            attn_add = attn_concat + input_trfm\n",
    "            attn_norm = tf.contrib.layers.layer_norm(attn_add, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "            attn_ffn = tf.nn.relu(tf.add(tf.tensordot(attn_norm, W_TRFM['WFFN1_layer_' + str(layer)], axes=1),\n",
    "                                         tf.tile(W_TRFM['BFFN1_layer_' + str(layer)],\n",
    "                                                 [self.batch_size, self.sequence_length, 1])))\n",
    "\n",
    "            attn_add2 = attn_norm + attn_ffn\n",
    "            attn_norm2 = tf.contrib.layers.layer_norm(attn_add2, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "\n",
    "            input_trfm = attn_norm2\n",
    "\n",
    "        return input_trfm\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        # doc id to doc index\n",
    "        self.item_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32,\n",
    "                                                                     default_value=-1, name=\"itemIdMap\",\n",
    "                                                                     checkpoint=True)\n",
    "        #self.tag_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32, default_value=-1, name=\"tagIdMap\", checkpoint=True)\n",
    "\n",
    "        # doc index to doc id\n",
    "        self.item_id_hash_table_reverse = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int64,\n",
    "                                                                             default_value=-1, name=\"itemIdMap\",\n",
    "                                                                             checkpoint=True)\n",
    "\n",
    "        self.input_item_id_tensor = tf.placeholder(tf.int64, [None, None], name='input_item_id_tensor')  # [batch_size, sequence_length]\n",
    "        #self.input_tag_id_tensor = tf.placeholder(tf.int64, [None, None],name='input_tag_id_tensor')  # [batch_size, sequence_length]\n",
    " \n",
    "        self.X_time_diff = tf.placeholder(tf.int32, [None, None], name='input_time_diff')  # [batch_size, sequence_length]\n",
    "\n",
    "        self.target_item_id_tensor = tf.placeholder(tf.int64, [None], name='target_item_id_tensor')  # [batch_size]\n",
    "\n",
    "        self.X = self.item_id_hash_table.lookup(self.input_item_id_tensor)\n",
    "        #self.X_tag = self.tag_id_hash_table.lookup(self.input_tag_id_tensor)\n",
    "        \n",
    "        #find neigbor item X \n",
    "        self.X_neig = tf.nn.embedding_lookup(self.adj_item_item_tensor, self.X+1) \n",
    "        adj_lists = tf.reshape(self.X_neig, [-1, self.max_degree])\n",
    "        adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        adj_lists = tf.slice(adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.X_neig = tf.reshape(adj_lists, [tf.shape(self.X)[0], tf.shape(self.X)[1], self.num_samples])\n",
    "        ###\n",
    "\n",
    "        self.Y = self.item_id_hash_table.lookup(self.target_item_id_tensor)\n",
    "        #find neigbor item Y\n",
    "        self.Y_expand = tf.expand_dims(self.Y, 0)\n",
    "        self.Y_neig = tf.nn.embedding_lookup(self.adj_item_item_tensor, self.Y_expand+1) \n",
    "        target_adj_lists = tf.reshape(self.Y_neig, [-1, self.max_degree])\n",
    "        target_adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(target_adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        target_adj_lists = tf.slice(target_adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.Y_neig = tf.reshape(target_adj_lists, [tf.shape(self.Y_expand)[0], tf.shape(self.Y_expand)[1], self.num_samples])\n",
    "        ###\n",
    "        ### find random item Y\n",
    "        self.random_Y_neig = tf.nn.embedding_lookup(self.random_item_item_tensor, self.Y_expand+1) \n",
    "        random_target_adj_lists = tf.reshape(self.random_Y_neig, [-1, self.max_degree])\n",
    "        random_target_adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(random_target_adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        random_target_adj_lists = tf.slice(random_target_adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.random_Y_neig = tf.reshape(random_target_adj_lists, [tf.shape(self.Y_expand)[0], tf.shape(self.Y_expand)[1], self.num_samples])\n",
    "        \n",
    "        \n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)  # global step can not be trained.\n",
    "\n",
    "        with tf.variable_scope('bert_layer'):\n",
    "            sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (self.n_items + self.rnn_size))\n",
    "            if self.init_as_normal:\n",
    "                initializer = tf.random_normal_initializer(mean=0, stddev=sigma)\n",
    "            else:\n",
    "                initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)  # for default\n",
    "\n",
    "            embedding = tf.get_variable('embedding', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            #embedding_tag = tf.get_variable('embedding_tag', [self.n_tags, self.rnn_size], initializer=initializer)\n",
    "         \n",
    "                                            \n",
    "            embedding_item_neigh_weights = tf.get_variable('item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            embedding_item_self_weights = tf.get_variable('item_self_weights', [self.rnn_size, self.rnn_size], initializer=initializer)                              \n",
    "            \n",
    "            target_embedding_item_neigh_weights = tf.get_variable('target_item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            target_embedding_item_self_weights = tf.get_variable('target_item_self_weights', [self.rnn_size, self.rnn_size], initializer=initializer)                              \n",
    "            #random_target_embedding_item_neigh_weights = tf.get_variable('random_target_embedding_item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            \n",
    "            embedding_pos = tf.get_variable('embedding_pos', [1, self.max_click, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            feat_W = tf.get_variable('feat_w', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            feat_b = tf.get_variable('feat_b', [self.rnn_size], initializer=initializer)\n",
    "\n",
    "\n",
    "            ###\n",
    "            user_W = tf.get_variable('user_w', [self.rnn_size * 2, self.rnn_size ], initializer=initializer)\n",
    "            \n",
    "            #user_W = tf.get_variable('user_w', [self.rnn_size * 1, self.rnn_size ], initializer=initializer)\n",
    "            user_b = tf.get_variable('user_b', [self.rnn_size ], initializer=initializer)\n",
    "\n",
    "            embedding_time_diff = tf.get_variable('embedding_time_diff', [self.time_emb_num, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            softmax_W = tf.get_variable('softmax_w', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            softmax_b = tf.get_variable('softmax_b', [self.n_items], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            self.W_TRFM = self.build_transformer(self.bert_layer, self.head_num, self.rnn_size, initializer)\n",
    "\n",
    "\n",
    "            ### input process\n",
    "            self.inputs_item = tf.nn.embedding_lookup(embedding, self.X + 1)  # [batch_size, sequence_length, rnn_size]\n",
    "            #add item neighor aggregator\n",
    "            #self.inputs_item_tmp = tf.nn.leaky_relu(tf.tensordot(self.inputs_item, embedding_item_self_weights, axes=1), alpha=0.2)\n",
    "            self.inputs_item_tmp = tf.tensordot(self.inputs_item, embedding_item_self_weights, axes=1)\n",
    "            self.inputs_item_tmp = tf.expand_dims(self.inputs_item_tmp, 2) # [batch_size, sequence_length, 1, rnn_size]\n",
    "            \n",
    "            self.inputs_item_neig = tf.nn.embedding_lookup(embedding, self.X_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            #self.inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.inputs_item_neig, embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            self.inputs_item_neig = tf.tensordot(self.inputs_item_neig, embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            self.inputs_item_attn = tf.squeeze(tf.matmul(tf.nn.softmax(tf.matmul(self.inputs_item_tmp, tf.transpose(self.inputs_item_neig, perm=[0, 1, 3, 2])) / tf.sqrt(tf.cast(self.rnn_size, tf.float32))), self.inputs_item_neig), [2])\n",
    "            \n",
    "            self.inputs_item =  tf.add_n([self.inputs_item, self.inputs_item_attn])\n",
    "            ###\n",
    "            \n",
    "            #inputs_tag = tf.nn.embedding_lookup(embedding_tag, self.X_tag + 1)  #\n",
    "\n",
    "            # [batch_size, sequence_length, rnn_size * 2]\n",
    "            #inputs_feat = tf.concat([self.inputs_item, inputs_tag], 2)\n",
    "            inputs_feat = self.inputs_item\n",
    "\n",
    "            # [batch_size, sequence_length, rnn_size]\n",
    "            self.inputs_time_diff = tf.nn.embedding_lookup(embedding_time_diff, self.X_time_diff + 1)\n",
    "            \n",
    "            ###\n",
    "            self.inputs_tmp = tf.nn.tanh( tf.add(tf.tensordot(inputs_feat, feat_W, axes=1), feat_b)) + self.inputs_time_diff\n",
    "            \n",
    "            #self.inputs_tmp = tf.nn.tanh( tf.add(tf.tensordot(inputs_feat, feat_W, axes=1), feat_b)) \n",
    "\n",
    "            self.inputs = tf.reshape(self.inputs_tmp, [self.batch_size, -1, self.rnn_size])\n",
    "\n",
    "            embedding_pos_slice = tf.slice(embedding_pos, [0, 0, 0], [1, self.sequence_length, self.rnn_size])\n",
    "            embedding_pos_rev = tf.reverse(embedding_pos_slice, [1])\n",
    "\n",
    "            self.inputs_pe = tf.add(self.inputs, tf.tile(embedding_pos_rev, [self.batch_size, 1, 1]))\n",
    "\n",
    "            self.trfm_input_um = self.inputs_pe\n",
    "\n",
    "            self.trfm_out  = self.run_transformer(self.bert_layer, self.head_num, self.rnn_size, self.trfm_input_um, self.W_TRFM)\n",
    "            \n",
    "            self.trfm_out   = tf.nn.dropout(self.trfm_out, keep_prob=self.dropout_p_hidden)\n",
    "            \n",
    "            ###\n",
    "            self.trfm_final = tf.slice(self.trfm_out, [0, self.sequence_length - 2, 0], [self.batch_size, 2, self.rnn_size])\n",
    "            #self.trfm_final = tf.slice(self.trfm_out, [0, self.sequence_length - 1, 0], [self.batch_size, 1, self.rnn_size])\n",
    "            self.final_state = tf.reshape(self.trfm_final, [self.batch_size, -1])\n",
    "            # self.final_state = tf.squeeze(self.trfm_final)\n",
    "\n",
    "        self.user_vec1 = tf.reshape(self.final_state, [self.batch_size, -1])\n",
    "        \n",
    "        self.user_final = tf.nn.leaky_relu(tf.add(tf.matmul(self.user_vec1, user_W), user_b))\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            #sampled_W = tf.nn.embedding_lookup(softmax_W, self.Y + 1)\n",
    "            #sampled_b = tf.nn.embedding_lookup(softmax_b, self.Y + 1)\n",
    "            sampled_W = tf.nn.embedding_lookup(embedding, self.Y + 1)\n",
    "            sampled_b = tf.nn.embedding_lookup(softmax_b, self.Y + 1)\n",
    "            \n",
    "            ### target process\n",
    "            self.Y_expand = tf.expand_dims(self.Y, 0)\n",
    "            self.target_inputs_item = tf.nn.embedding_lookup(embedding, self.Y_expand + 1)  # [batch_size, sequence_length, rnn_size]\n",
    "            self.target_inputs_item_tmp = tf.nn.leaky_relu(tf.tensordot(self.target_inputs_item, target_embedding_item_self_weights, axes=1), alpha=0.2)\n",
    "            #self.target_inputs_item_tmp = tf.tensordot(self.target_inputs_item, target_embedding_item_self_weights, axes=1)\n",
    "            self.target_inputs_item_tmp = tf.expand_dims(self.target_inputs_item_tmp, 2) # [batch_size, sequence_length, 1, rnn_size]\n",
    "            \n",
    "            \n",
    "            self.target_inputs_item_neig = tf.nn.embedding_lookup(embedding, self.Y_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            self.target_inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            #self.target_inputs_item_neig = tf.tensordot(self.target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            self.random_target_inputs_item_neig = tf.nn.embedding_lookup(embedding, self.random_Y_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            self.random_target_inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.random_target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            #self.random_target_inputs_item_neig = tf.tensordot(self.random_target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            #self.target_inputs_item_attn = tf.squeeze(tf.matmul(tf.nn.softmax(tf.matmul(self.target_inputs_item_tmp, tf.transpose(self.target_inputs_item_neig, perm=[0, 1, 3, 2])) / tf.sqrt(tf.cast(self.rnn_size, tf.float32))), self.target_inputs_item_neig), [2])\n",
    "            self.target_inputs_item_attn = tf.reduce_sum(self.target_inputs_item_neig,[2])\n",
    "            self.random_target_inputs_item_attn = tf.reduce_sum(self.random_target_inputs_item_neig,[2])\n",
    "            \n",
    "            #self.target_inputs_item =  tf.add_n([self.target_inputs_item, self.target_inputs_item_attn])\n",
    "            self.target_inputs_item =  self.target_inputs_item_attn\n",
    "            \n",
    "            ###\n",
    "            #self.target_inputs_item = tf.squeeze(self.target_inputs_item, [0])\n",
    "            #sampled_W =  tf.add_n([sampled_W, self.target_inputs_item])\n",
    "            \n",
    "            self.logits = tf.matmul(self.user_final, sampled_W, transpose_b=True) + sampled_b\n",
    "            self.contrastive_logits = tf.matmul(self.user_final, self.target_inputs_item, transpose_b=True) + sampled_b\n",
    "            self.random_contrastive_logits = tf.matmul(self.user_final, self.random_target_inputs_item_attn, transpose_b=True) + sampled_b\n",
    "            \n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "            self.contrastive_yhat = tf.squeeze(self.final_activation(self.contrastive_logits), [0])\n",
    "            self.random_contrastive_yhat = tf.squeeze(self.final_activation(self.random_contrastive_logits), [0])\n",
    "            \n",
    "            self.target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            #self.target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            #self.random_target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.contrastive_yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            \n",
    "            self.cost = self.loss_function(self.yhat) + self.target_cost \n",
    "            #self.cost = self.loss_function(self.yhat) \n",
    "            \n",
    "        else:\n",
    "            softmax_W_topN = tf.slice(softmax_W, [0, 0], [self.predictTopN, self.rnn_size])\n",
    "            softmax_b_topN = tf.slice(softmax_b, [0], [self.predictTopN])\n",
    "            self.logits = tf.matmul(self.user_final, softmax_W_topN, transpose_b=True) + softmax_b_topN\n",
    "            # self.logits = tf.matmul(self.user_vec2, softmax_W, transpose_b=True) + softmax_b\n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "\n",
    "            # ## savedModel ## #\n",
    "            self.yhat = tf.identity(self.yhat, name=\"yhat\")\n",
    "\n",
    "            # topNum = tf.divide(tf.constant(1000), tf.max(tf.constant(1), outSeqNum))\n",
    "            self.topk_prob_sample, self.topk_ind_sample = tf.nn.top_k(self.yhat, 4000, name=\"topk\")\n",
    "\n",
    "            self.topk_ind_sample = self.topk_ind_sample - 1\n",
    "\n",
    "            self.topk_doc_id = self.item_id_hash_table_reverse.lookup(tf.cast(self.topk_ind_sample, tf.int64))\n",
    "\n",
    "            topk_prob_sample_1d = tf.reshape(self.topk_prob_sample, [-1])\n",
    "            topk_ind_sample_1d = tf.reshape(self.topk_ind_sample, [-1])\n",
    "            topk_doc_id_1d = tf.reshape(self.topk_doc_id, [-1])\n",
    "\n",
    "            self.topk_ind_val = tf.add(topk_ind_sample_1d, tf.constant(0), name=\"topk_ind_val\")\n",
    "            self.topk_prob_val = tf.add(topk_prob_sample_1d, tf.constant(0.0), name=\"topk_prob_val\")\n",
    "            self.topk_doc_id = tf.identity(topk_doc_id_1d, name=\"topk_doc_id\")\n",
    "            return\n",
    "\n",
    "        self.lr = tf.maximum(1e-5, tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n",
    "                                                              self.decay, staircase=True))\n",
    "\n",
    "        ''' Try different optimizers. '''\n",
    "        # optimizer = tf.train.AdagradOptimizer(self.lr)\n",
    "        # optimizer = tf.train.AdadeltaOptimizer(self.lr)\n",
    "        # optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        gvs = optimizer.compute_gradients(self.cost, tvars)\n",
    "        if self.grad_cap > 0:\n",
    "            capped_gvs = [(tf.clip_by_norm(grad, self.grad_cap), var) for grad, var in gvs]\n",
    "        else:\n",
    "            capped_gvs = gvs\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs, global_step=self.global_step)\n",
    "        \n",
    "    def save_obj(self, path, obj, name):\n",
    "        with open(path + name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(self, path, name):\n",
    "        with open(path + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "            \n",
    "    def preprocess(self, verbose=False):\n",
    "\n",
    "        \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write( \"{} begin preprocessing...\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) + '\\n')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('start loading file: train_file: ' + self.train_path + self.ori_file + '\\n')\n",
    "\n",
    "        #         try:\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('loading raw data. \\n')\n",
    "            \n",
    "        mv_data_pd = pd.read_pickle(self.base_path + self.ori_file+\".pkl\")\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('finish loading raw data. \\n')\n",
    "\n",
    "        \n",
    "\n",
    "        # get expand data\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting expand data. \\n')\n",
    "\n",
    "        mv_data_pd['Sequence'] = mv_data_pd['Sequence'].astype('str')\n",
    "        \n",
    "        data_expand = pd.concat([Series(row['SessionId'],row['Sequence'].split('#')) for _, row in mv_data_pd.iterrows()]).reset_index()\n",
    "                                 \n",
    "        data_expand.columns = ['ItemInfo', 'SessionId']\n",
    "        data_expand[['ItemId', 'Time' ]] = pd.DataFrame( data_expand.ItemInfo.str.split('_').tolist())\n",
    "\n",
    "        # get item, tag, subject id-idx dictionary dataframe\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting item, tag, subject id-idx dictionary. \\n')\n",
    "\n",
    "        ItemIdCount = data_expand['ItemId'].value_counts()\n",
    "        ItemIdFreq = ItemIdCount.keys().tolist()\n",
    "        ItemIdFreqPd = pd.DataFrame(ItemIdFreq)\n",
    "        ItemIdFreqPd.columns = ['ItemId']\n",
    "        ItemIdPd = ItemIdFreqPd\n",
    "        ItemIdPd['ItemIdx'] = range(len(ItemIdFreq))\n",
    "        ItemIdPd.columns = ['ItemId', 'ItemIdx']\n",
    "\n",
    "        data_ItemInfo = data_expand[['ItemId']]\n",
    "        data_ItemInfo.drop_duplicates(inplace=True)\n",
    "        # data_ItemInfo = pd.merge(data_ItemInfo, ItemIdFreqPd, on='ItemId', how='inner')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting feature data. \\n')\n",
    "            \n",
    "            \n",
    "        # get feature data\n",
    "        data_ItemInfo.dropna(inplace=True)\n",
    "        \n",
    "        data = data_expand[['SessionId', 'ItemId', 'Time']]\n",
    "        data = pd.merge(data, data_ItemInfo, on='ItemId', how='inner')\n",
    "        \n",
    "        data = pd.merge(data, mv_data_pd[['SessionId','Cluster']], on='SessionId', how='inner')\n",
    "\n",
    "        \n",
    "        data = data.sort_values(by=['SessionId', 'Time'])\n",
    "        #data = data.sort_values(by=['SessionId'])\n",
    "        \n",
    "        data = data[data['ItemId'] != '']\n",
    "        data = data[data['Time'] != '']\n",
    "        \n",
    "        data.ItemId = data.ItemId.astype(int)\n",
    "        data.Time = data.Time.astype(int)\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting offset_sessions. \\n')\n",
    "\n",
    "        # get offset_sessions\n",
    "        offset_sessions = np.zeros(data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('saving data by pickle. \\n')\n",
    "\n",
    "        # save obj\n",
    "        \n",
    "        self.save_obj(self.train_path, offset_sessions, self.ori_file + '_offset_sessions')\n",
    "        self.save_obj(self.train_path, data, self.ori_file + '_data')\n",
    "        self.save_obj(self.train_path, ItemIdPd, self.ori_file + '_ItemIdPd')\n",
    "        \n",
    "        try:\n",
    "            a = 1/0\n",
    "            G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            grouped = data.groupby('SessionId') \n",
    "            with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('saving meta path edgelists. \\n')\n",
    "                \n",
    "            with open(self.train_path + self.ori_file + '_item_item_edge_list.txt', 'w') as f:\n",
    "                for _, g_i in grouped:\n",
    "                    pair_list = g_i.sort_values(by=['Time'])['ItemId'].tolist()\n",
    "                    if len(pair_list) == 1:\n",
    "                        f.write(\"{} {}\\n\".format(str(pair_list[0]),str(pair_list[0])))\n",
    "                    else:\n",
    "                        window_size = 5\n",
    "                        for l in range(len(pair_list)-1):\n",
    "                            for m in range(l-window_size, l+window_size+1):\n",
    "                                if m<0 or m>=len(pair_list): continue\n",
    "                                f.write(\"{} {}\\n\".format(str(pair_list[l]),str(pair_list[m])))\n",
    "                    \n",
    "                        #for i in range(len(pair_list)-1):\n",
    "                            #f.write(\"{} {}\\n\".format(str(pair_list[i]),str(pair_list[i+1])))\n",
    "                f.close()\n",
    "                \n",
    "\n",
    "    def enrich_edges(self):\n",
    "        G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        G_tag_tag = nx.read_edgelist(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', nodetype=int)\n",
    "        G_item_tag = nx.read_edgelist(self.train_path + self.ori_file + '_item_tag_edge_list.txt', nodetype=int)\n",
    "        \n",
    "        data = self.load_obj(self.train_path, self.ori_file + '_data')\n",
    "        \n",
    "        Set_item_id = set(data['ItemId'].tolist())\n",
    "        Set_item_idx = {}\n",
    "        Set_item_idx_reverse = {}\n",
    "        Set_tag_id = set(data['TagId'].tolist())\n",
    "        Set_tag_idx = {}\n",
    "        Set_tag_idx_reverse = {}\n",
    "\n",
    "        for i,j in enumerate(Set_item_id):\n",
    "            Set_item_idx[j] = i\n",
    "            Set_item_idx_reverse[i] = j\n",
    "        for i,j in enumerate(Set_tag_id):\n",
    "            Set_tag_idx[j] = i\n",
    "            Set_tag_idx_reverse[i] = j\n",
    "            \n",
    "        seq_adj_item_item = np.ones((len(Set_item_id), self.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[i] for i in list(G_item_item.neighbors(node_id))] )\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_item_item[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_tag = np.ones((len(Set_tag_id), self.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[i] for i in list(G_tag_tag.neighbors(node_id))] )\n",
    "            \n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_tag_tag[Set_tag_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_item_tag = np.ones((len(Set_item_id), bert.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[int(i.split(\"_\")[0])] for i in list(G_item_tag.neighbors(str(node_id)))] )\n",
    "            if len(neighbors) > bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=False)\n",
    "            elif len(neighbors) < bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=True)\n",
    "            seq_adj_item_tag[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_item = np.ones((len(Set_tag_id), bert.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[int(i)] for i in list(G_item_tag.neighbors(str(node_id)+\"_tag\"))] )\n",
    "            if len(neighbors) > bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=False)\n",
    "            elif len(neighbors) < bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=True)\n",
    "            seq_adj_tag_item[Set_tag_idx[node_id],:] = neighbors\n",
    "          \n",
    "\n",
    "        \n",
    "            \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('enrich meta path edgelists. \\n')\n",
    "                \n",
    "        print('enrich meta path edgelists')\n",
    "                \n",
    "        with open(self.train_path + self.ori_file + '_item_item_edge_list.txt', 'a') as f:\n",
    "            for item in G_item_item.nodes():\n",
    "                for item_i in range(G_item_item.degree(item)):\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[item]])]\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[random_tag_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(item_i),str(random_item_id)))\n",
    "            f.close()\n",
    "        \n",
    "        with open(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', 'a') as f:\n",
    "            for tag in G_tag_tag.nodes():\n",
    "                for tag_i in range(G_tag_tag.degree(tag)):\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[tag]] )]\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[random_item_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(tag),str(random_tag_id)))\n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "\n",
    "    def loaddata(self, verbose=False):\n",
    "        # load obj\n",
    "        print('loading data by pickle. \\n')\n",
    "\n",
    "        train_path = self.train_path\n",
    "        ori_file = self.ori_file\n",
    "        self.offset_sessions = self.load_obj(train_path, ori_file + '_offset_sessions')\n",
    "        self.data = self.load_obj(train_path, ori_file + '_data')\n",
    "        self.ItemIdPd = self.load_obj(train_path, ori_file + '_ItemIdPd')\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != '']\n",
    "\n",
    "        self.ItemIdPd.dropna(inplace=True)\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != 'nan']\n",
    "\n",
    "        self.ItemIdList = [int(k) for k in self.ItemIdPd.ItemId.tolist()]\n",
    "        self.ItemIdxList = [int(k) for k in self.ItemIdPd.ItemIdx.tolist()]\n",
    "\n",
    "\n",
    "        #test_cut = int(self.data.shape[0] * 1 / 2)\n",
    "        #self.data = self.data.iloc[test_cut:-1]\n",
    "        #self.data_test = self.data.iloc[0:test_cut]\n",
    "        \n",
    "        item_dict={}\n",
    "        for i,j in zip(self.ItemIdList, self.ItemIdxList):\n",
    "            item_dict[i] = j\n",
    "            \n",
    "        self.G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        print(\"start convert item G to tensor\")\n",
    "        self.adj_item_item = np.zeros((len(self.ItemIdList)+2, self.max_degree))\n",
    "        for node_id in self.ItemIdList:\n",
    "            if self.G_item_item.has_node(node_id):\n",
    "                neighbors = np.array( [item_dict[i] for i in list(self.G_item_item.neighbors(node_id))] )\n",
    "\n",
    "                if len(neighbors) > self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "                elif len(neighbors) < self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            else:\n",
    "                neighbors = np.ones((1,self.max_degree)) * (item_dict[node_id])\n",
    "                \n",
    "            self.adj_item_item[item_dict[node_id]+1,:] = neighbors\n",
    "            \n",
    "        self.adj_item_item_tensor = tf.convert_to_tensor(self.adj_item_item, dtype = tf.int32)  \n",
    "        \n",
    "        ### \n",
    "        self.random_item_item = np.zeros((len(self.ItemIdList)+2, self.max_degree))\n",
    "        neighbors = np.array( [item_dict[i] for i in list(self.G_item_item)] )\n",
    "        for node_id in self.ItemIdList:\n",
    "            if self.G_item_item.has_node(node_id):\n",
    "                if len(neighbors) > self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "                elif len(neighbors) < self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            else:\n",
    "                neighbors = np.ones((1,self.max_degree)) * (item_dict[node_id])\n",
    "                \n",
    "            self.random_item_item[item_dict[node_id]+1,:] = neighbors\n",
    "            \n",
    "        self.random_item_item_tensor = tf.convert_to_tensor(self.random_item_item, dtype = tf.int32)  \n",
    "        \n",
    "\n",
    "    def toFloat(self, inVal):\n",
    "        try:\n",
    "            outVal = float(inVal)\n",
    "        except:\n",
    "            outVal = 0.0\n",
    "\n",
    "        if np.isnan(outVal):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return outVal\n",
    "\n",
    "        \n",
    "    def test(self):\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "        \n",
    "        self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        session_idx_arr = np.arange(len(self.offset_sessions) - 1)\n",
    "        \n",
    "        self.session_embedding_matrix = np.zeros([len(self.offset_sessions), self.rnn_size])\n",
    "        self.session_cluster_matrix = np.zeros([len(self.offset_sessions), 1])\n",
    "        \n",
    "        self.session_words_matrix = np.zeros([len(self.offset_sessions), self.rnn_size])\n",
    "        \n",
    "        self.sess_doc_array = []\n",
    "        self.sess_cluster_array = []\n",
    "        grouped = self.data.groupby('SessionId')\n",
    "        for _, g_i in grouped:\n",
    "            self.sess_doc_array.append(g_i['SessionId'].tolist()[0])\n",
    "            self.sess_cluster_array.append(g_i['Cluster'].tolist()[0])\n",
    "            \n",
    "        for s_idx in session_idx_arr:\n",
    "            start = self.offset_sessions[s_idx]\n",
    "            end = self.offset_sessions[s_idx + 1] - 1\n",
    "            if end - start > 1:\n",
    "                pos_idx = end - 1\n",
    "                if self.sequence_length < end - start:\n",
    "                    end = start + self.sequence_length\n",
    "                    pos_idx = end - 1\n",
    "                self.selected_sess = np.arange(start, pos_idx)\n",
    "                self.len_selected_sess = len(self.selected_sess)\n",
    "                \n",
    "                self.in_idx = np.array(self.data.ItemId.values[self.selected_sess].tolist())\n",
    "                self.out_idx = np.array(self.data.ItemId.values[end])\n",
    "                \n",
    "                self.in_time = self.data.Time.values[self.selected_sess].tolist()\n",
    "                self.out_time = self.data.Time.values[self.selected_sess+1].tolist()\n",
    "                \n",
    "                self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.in_time))]\n",
    "                self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                \n",
    "                self.in_idx_matrix = np.zeros([1, self.sequence_length], dtype=int)\n",
    "                self.in_time_diff_matrix = np.zeros([1, self.sequence_length], dtype=int) + 28\n",
    "        \n",
    "                self.in_idx_matrix = np.concatenate([np.reshape(self.in_idx,[1,-1]), self.in_idx_matrix[:, self.len_selected_sess:]],1)\n",
    "                self.in_time_diff_matrix = np.concatenate([np.reshape(self.in_time_diff,[1,-1]), self.in_time_diff_matrix[:, self.len_selected_sess:]],1) \n",
    "                \n",
    "                def flip_swap_zero(matrix_in, fillval=0):\n",
    "                    matrix_2d = []\n",
    "                    for batch_cnt in range(matrix_in.shape[0]):\n",
    "                        tmp_val = matrix_in[batch_cnt]\n",
    "                        tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                        matrix_2d.append(np.concatenate(\n",
    "                            [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                    return matrix_2d\n",
    "                    \n",
    "                \n",
    "                self.in_idx_matrix_swap = flip_swap_zero(self.in_idx_matrix)\n",
    "                \n",
    "                self.in_time_diff_matrix_swap = flip_swap_zero(self.in_time_diff_matrix, fillval=28)\n",
    "                \n",
    "                \n",
    "                feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(self.in_idx_matrix_swap),\n",
    "                                 \n",
    "                                  self.X_time_diff: np.nan_to_num(self.in_time_diff_matrix_swap),\n",
    "\n",
    "                                  #self.target_item_id_tensor: [np.nan_to_num(np.array(self.out_idx))],\n",
    "\n",
    "                                  \n",
    "                                  self.batch_size: 1\n",
    "                                  }\n",
    "                \n",
    "                \n",
    "                fetches_test = [self.user_final, self.inputs_item]\n",
    "                self.batch_user_final, self.user_words_emb = self.sess.run(fetches_test, feed_dict_test)\n",
    "                self.session_embedding_matrix[int(self.sess_doc_array[s_idx]),:] = self.batch_user_final\n",
    "                self.session_cluster_matrix[int(self.sess_doc_array[s_idx])] = self.sess_cluster_array[s_idx]\n",
    "                \n",
    "                #print(self.user_words_emb.shape)\n",
    "                self.session_words_matrix[int(self.sess_doc_array[s_idx])] = np.mean(self.user_words_emb, axis=1)\n",
    "        \n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "\n",
    "        def sigmoid(input_val):\n",
    "            return 1 / (1 + (math.e ** -input_val))\n",
    "\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            \n",
    "\n",
    "            self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "            self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "            cost_train_list = []\n",
    "            cost_test_list = []\n",
    "            cut_cost_test_list = []\n",
    "\n",
    "            auc_train_list = []\n",
    "            auc_test_list = []\n",
    "            cut_auc_test_list = []\n",
    "            \n",
    "\n",
    "            session_idx_arr = np.arange(len(self.offset_sessions) - 1)\n",
    "            iters = np.arange(self.batch_size_pos)\n",
    "            maxiter = iters.max()  # self.batch_size - 1\n",
    "            # iters is a array, so start will return a array with size of batch_size\n",
    "            start = self.offset_sessions[session_idx_arr[iters]]\n",
    "            end = self.offset_sessions[session_idx_arr[iters] + 1]\n",
    "            finished = False\n",
    "            \n",
    "            max_test_auc = 0\n",
    "            tolerant_time = self.tolerant_time\n",
    "\n",
    "            train_global_step = 0\n",
    "            step = 0\n",
    "\n",
    "            in_idx_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int)\n",
    "            #in_tag_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int)\n",
    "            in_time_diff_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int) + 28\n",
    "\n",
    "            while not finished:\n",
    "                minlen = (end - start).min()\n",
    "                out_idx = self.data.ItemId.values[start]\n",
    "                #out_tag = self.data.TagId.values[start]\n",
    "                out_sessionId = self.data.SessionId.values[start]\n",
    "                out_time = self.data.Time.values[start]\n",
    "                \n",
    "                ###\n",
    "                for i in range(minlen - 1):\n",
    "\n",
    "                    pos_idx = start + i\n",
    "                    in_idx = self.data.ItemId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    #in_tag = self.data.TagId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    in_sessionId = self.data.SessionId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "\n",
    "                    train_idx = start + i + 1\n",
    "                    out_idx = self.data.ItemId.values[train_idx]\n",
    "\n",
    "                    # build time diff\n",
    "                    self.in_time = self.data.Time.values[pos_idx].tolist()\n",
    "                    self.out_time = self.data.Time.values[train_idx].tolist()\n",
    "                    \n",
    "                    self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.out_time))]\n",
    "                    self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                    self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                    self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                   \n",
    "\n",
    "                    in_idx_matrix = np.concatenate([np.reshape(np.array(in_idx), [-1, 1]), in_idx_matrix[:, 0:-1]], 1)\n",
    "                    #in_tag_matrix = np.concatenate([np.reshape(np.array(in_tag), [-1, 1]), #in_tag_matrix[:, 0:-1]], 1)\n",
    "                    \n",
    "                    #self.in_idx_matrix = in_idx_matrix\n",
    "                    \n",
    "                    in_time_diff_matrix = np.concatenate([np.reshape(np.array(self.in_time_diff),\n",
    "                                                                     [-1, 1]), in_time_diff_matrix[:, 0:-1]], 1)\n",
    "\n",
    "                    def flip_swap_zero(matrix_in, fillval=0):\n",
    "                        matrix_2d = []\n",
    "                        for batch_cnt in range(matrix_in.shape[0]):\n",
    "                            tmp_val = matrix_in[batch_cnt]\n",
    "                            tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                            matrix_2d.append(np.concatenate(\n",
    "                                [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                        return matrix_2d\n",
    "                    \n",
    "                    #self.test_in_idx_matrix = in_idx_matrix\n",
    "                    #self.test_in_idx = in_idx\n",
    "                    \n",
    "                    \n",
    "                    in_idx_matrix_swap = flip_swap_zero(in_idx_matrix)\n",
    "                    #in_tag_matrix_swap = flip_swap_zero(in_tag_matrix)\n",
    "                    \n",
    "                    in_time_diff_matrix_swap = flip_swap_zero(in_time_diff_matrix, fillval=28)\n",
    "\n",
    "                   \n",
    "                    \n",
    "                    num_to_select = int(len(in_sessionId) * 4.0 / 5)\n",
    "                    \n",
    "                    \n",
    "                    train_set_index = list(np.arange(num_to_select))\n",
    "                    batch_size_train = len(train_set_index)\n",
    "                    \n",
    "                    \n",
    "                    test_set_index = list(np.arange(num_to_select, len(in_sessionId)))\n",
    "                    batch_size_test = len(test_set_index)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    self.item_tmp = np.nan_to_num(in_idx_matrix_swap)[train_set_index]\n",
    "                    #self.tag_tmp = np.nan_to_num(in_tag_matrix_swap)[train_set_index]\n",
    "                    \n",
    "                    feed_dict = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[train_set_index], self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[train_set_index], self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[train_set_index], self.batch_size: batch_size_train}\n",
    "                    fetches = [self.yhat, self.cost, self.final_state, self.global_step, self.lr, self.train_op]\n",
    "                    #                     fetches = [self.trfm_out]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    train_global_step += 1\n",
    "                    yhat, cost, state, step, lr, _ = self.sess.run(fetches, feed_dict)\n",
    "                    #                     self.trfm_out_check = self.sess.run(fetches, feed_dict)\n",
    "                    #                     print(self.trfm_out_check)\n",
    "\n",
    "                    #                     return\n",
    "                    \n",
    "\n",
    "                    # evaluate training result\n",
    "                    fetches = [self.yhat, self.cost]\n",
    "                    yhat_train, cost_train = self.sess.run(fetches, feed_dict)\n",
    "\n",
    "\n",
    "                    \n",
    "                    feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[test_set_index],self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[test_set_index], self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[test_set_index], self.batch_size: batch_size_test}\n",
    "\n",
    "                    fetches_test = [self.yhat, self.cost]\n",
    "                    yhat_test, cost_test = self.sess.run(fetches_test, feed_dict_test)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    if np.isnan(cost):\n",
    "                        print(str(epoch) + ':Nan error!')\n",
    "                        self.error_during_train = True\n",
    "                        return\n",
    "\n",
    "                    cost_train_list.append(cost_train)\n",
    "\n",
    "                    label_train = np.eye(yhat_train.shape[0])\n",
    "                    self.yhat_train = yhat_train\n",
    "                    self.label_train = label_train\n",
    "                    label_1d_train = np.reshape(label_train, [-1])\n",
    "                    prob_1d_train = np.reshape(yhat_train, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_train, prob_1d_train, pos_label=1)\n",
    "                    auc_train = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    label_test = np.eye(yhat_test.shape[0])\n",
    "                    label_1d_test = np.reshape(label_test, [-1])\n",
    "                    prob_1d_test = np.reshape(yhat_test, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_test, prob_1d_test, pos_label=1)\n",
    "                    auc_test = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    auc_train_list.append(auc_train)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if step % 20 == 0:\n",
    "                        #cost_test, auc_test = self.test_data()\n",
    "                        \n",
    "                        auc_test_list.append(auc_test)\n",
    "                        cost_test_list.append(cost_test)\n",
    "                        \n",
    "                        avgc = np.mean(cost_train_list)\n",
    "                        avgc_test = np.mean(cost_test_list)\n",
    "                        auc_train_avg = np.mean(auc_train_list)\n",
    "                        auc_test_avg = np.mean(auc_test_list)\n",
    "\n",
    "                        print('{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "\n",
    "                        if step > 10:\n",
    "                            cost_train_list = []\n",
    "                            cost_test_list = []\n",
    "                            auc_train_list = []\n",
    "                            auc_test_list = []\n",
    "                \n",
    "                        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                            log_file.write( '{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "                            \n",
    "                            if max_test_auc > auc_test_avg:\n",
    "                                if epoch > 0:\n",
    "                                    tolerant_time -= 1\n",
    "                                if tolerant_time == 0:\n",
    "                                    finished = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                max_test_auc = auc_test_avg\n",
    "                                tolerant_time = self.tolerant_time\n",
    "                                self.saver.save(self.sess, '{}/bert-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "\n",
    "                start = start + minlen - 1\n",
    "                mask = np.arange(len(iters))[(end - start) <= 1]  # idx of ended sessions\n",
    "                for idx in mask:\n",
    "                    maxiter += 1\n",
    "                    if maxiter >= len(self.offset_sessions) - 1:\n",
    "                        finished = True\n",
    "                        break\n",
    "                    iters[idx] = maxiter\n",
    "                    start[idx] = self.offset_sessions[session_idx_arr[maxiter]]\n",
    "                    end[idx] = self.offset_sessions[session_idx_arr[maxiter] + 1]\n",
    "\n",
    "                if len(mask) and self.reset_after_session:\n",
    "                    in_idx_matrix[mask, :] = 0\n",
    "                    #in_tag_matrix[mask, :] = 0\n",
    "                  \n",
    "                    in_time_diff_matrix[mask, :] = 28\n",
    "\n",
    "            if np.isnan(avgc):\n",
    "                print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "                with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                    log_file.write( '{} Epoch {}: Nan error!'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), epoch,\n",
    "                                                         avgc) + \"\\n\")\n",
    "                self.error_during_train = True\n",
    "                return\n",
    "            self.saver.save(self.sess, '{}/bert-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "            with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('ckpt to : ' + self.checkpoint_dir)\n",
    "        '''        \n",
    "        cost_test, auc_test = self.test_data()\n",
    "        cut_auc_test_list.append(auc_test)\n",
    "        cut_cost_test_list.append(cost_test)\n",
    "        cut_avgc = np.mean(cut_cost_test_list)\n",
    "        cut_avgauc = np.mean(cut_auc_test_list)\n",
    "        print('Ori_file{}, Epoch{},cutLossTst:{:.5f},cutAucTst:{:.5f}\\n'.format(\n",
    "                                    self.ori_file, epoch, cut_avgc, cut_avgauc))\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del bert\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used\n",
    "\n",
    "#with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "args.neg_pos_sam_rat = 7\n",
    "\n",
    "#bert.loaddata()\n",
    "#bert.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data by pickle. \n",
      "\n",
      "start convert item G to tensor\n"
     ]
    }
   ],
   "source": [
    "bert = BERT4Rec(sess, args)\n",
    "#bert.preprocess()\n",
    "bert.loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-0b30c8c5f28a>:378: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "init1\n",
      "init2\n",
      "init3\n"
     ]
    }
   ],
   "source": [
    "bert.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-06 22:54:27 Ori_fileTrends-T-new_process, Epoch0,Step20,lr:0.001000,loss:7.11127,auc:0.51269,lossTst:5.72981,aucTst:0.52905,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:54:36 Ori_fileTrends-T-new_process, Epoch0,Step40,lr:0.001000,loss:7.08024,auc:0.57819,lossTst:5.69438,aucTst:0.56673,MaxaucTst:0.52905,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:54:44 Ori_fileTrends-T-new_process, Epoch0,Step60,lr:0.001000,loss:7.03337,auc:0.59280,lossTst:5.66886,aucTst:0.59575,MaxaucTst:0.56673,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:54:51 Ori_fileTrends-T-new_process, Epoch0,Step80,lr:0.001000,loss:6.97247,auc:0.62764,lossTst:5.59588,aucTst:0.58716,MaxaucTst:0.59575,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:54:57 Ori_fileTrends-T-new_process, Epoch0,Step100,lr:0.001000,loss:6.92882,auc:0.64914,lossTst:5.68868,aucTst:0.58511,MaxaucTst:0.59575,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:03 Ori_fileTrends-T-new_process, Epoch0,Step120,lr:0.001000,loss:6.88511,auc:0.66452,lossTst:5.59694,aucTst:0.61306,MaxaucTst:0.59575,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:11 Ori_fileTrends-T-new_process, Epoch0,Step140,lr:0.001000,loss:6.83392,auc:0.68638,lossTst:5.47517,aucTst:0.69201,MaxaucTst:0.61306,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:19 Ori_fileTrends-T-new_process, Epoch0,Step160,lr:0.001000,loss:6.71525,auc:0.73043,lossTst:5.12884,aucTst:0.79513,MaxaucTst:0.69201,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:28 Ori_fileTrends-T-new_process, Epoch0,Step180,lr:0.001000,loss:6.64170,auc:0.75342,lossTst:5.29881,aucTst:0.74091,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:34 Ori_fileTrends-T-new_process, Epoch0,Step200,lr:0.001000,loss:6.62643,auc:0.75196,lossTst:5.42525,aucTst:0.69097,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:40 Ori_fileTrends-T-new_process, Epoch0,Step220,lr:0.001000,loss:6.61353,auc:0.75051,lossTst:5.29855,aucTst:0.72649,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:45 Ori_fileTrends-T-new_process, Epoch0,Step240,lr:0.001000,loss:6.64169,auc:0.74081,lossTst:5.33083,aucTst:0.73520,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:52 Ori_fileTrends-T-new_process, Epoch0,Step260,lr:0.001000,loss:6.61340,auc:0.74431,lossTst:5.37227,aucTst:0.71968,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:55:58 Ori_fileTrends-T-new_process, Epoch0,Step280,lr:0.001000,loss:6.62961,auc:0.74085,lossTst:5.38860,aucTst:0.70359,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:04 Ori_fileTrends-T-new_process, Epoch0,Step300,lr:0.001000,loss:6.56891,auc:0.75150,lossTst:5.37965,aucTst:0.72309,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:10 Ori_fileTrends-T-new_process, Epoch0,Step320,lr:0.001000,loss:6.64018,auc:0.73482,lossTst:5.29092,aucTst:0.72323,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:16 Ori_fileTrends-T-new_process, Epoch0,Step340,lr:0.001000,loss:6.55310,auc:0.75671,lossTst:5.15589,aucTst:0.74758,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:22 Ori_fileTrends-T-new_process, Epoch0,Step360,lr:0.001000,loss:6.56074,auc:0.75145,lossTst:5.27518,aucTst:0.70493,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:28 Ori_fileTrends-T-new_process, Epoch0,Step380,lr:0.001000,loss:6.46311,auc:0.77025,lossTst:5.06540,aucTst:0.76113,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:34 Ori_fileTrends-T-new_process, Epoch0,Step400,lr:0.001000,loss:6.55164,auc:0.74727,lossTst:5.37937,aucTst:0.70706,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:40 Ori_fileTrends-T-new_process, Epoch0,Step420,lr:0.001000,loss:6.57591,auc:0.74784,lossTst:5.17531,aucTst:0.75098,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:45 Ori_fileTrends-T-new_process, Epoch0,Step440,lr:0.001000,loss:6.58475,auc:0.74582,lossTst:5.42190,aucTst:0.69009,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:51 Ori_fileTrends-T-new_process, Epoch0,Step460,lr:0.001000,loss:6.55049,auc:0.75468,lossTst:5.10684,aucTst:0.77049,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:56:58 Ori_fileTrends-T-new_process, Epoch0,Step480,lr:0.001000,loss:6.53251,auc:0.75781,lossTst:5.26423,aucTst:0.73102,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:04 Ori_fileTrends-T-new_process, Epoch0,Step500,lr:0.001000,loss:6.58656,auc:0.74324,lossTst:5.21364,aucTst:0.73484,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:11 Ori_fileTrends-T-new_process, Epoch0,Step520,lr:0.001000,loss:6.51745,auc:0.75986,lossTst:5.27447,aucTst:0.72878,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:17 Ori_fileTrends-T-new_process, Epoch0,Step540,lr:0.001000,loss:6.50039,auc:0.76497,lossTst:5.13006,aucTst:0.78905,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:23 Ori_fileTrends-T-new_process, Epoch0,Step560,lr:0.001000,loss:6.46567,auc:0.77100,lossTst:5.17160,aucTst:0.74559,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:29 Ori_fileTrends-T-new_process, Epoch0,Step580,lr:0.001000,loss:6.49326,auc:0.76612,lossTst:4.94438,aucTst:0.79842,MaxaucTst:0.79513,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:37 Ori_fileTrends-T-new_process, Epoch0,Step600,lr:0.001000,loss:6.51905,auc:0.75534,lossTst:5.20761,aucTst:0.74250,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:44 Ori_fileTrends-T-new_process, Epoch0,Step620,lr:0.001000,loss:6.50971,auc:0.76298,lossTst:5.20466,aucTst:0.73051,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:50 Ori_fileTrends-T-new_process, Epoch0,Step640,lr:0.001000,loss:6.51343,auc:0.76132,lossTst:5.01968,aucTst:0.77638,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:57:57 Ori_fileTrends-T-new_process, Epoch0,Step660,lr:0.001000,loss:6.51376,auc:0.75797,lossTst:5.15226,aucTst:0.74315,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:03 Ori_fileTrends-T-new_process, Epoch0,Step680,lr:0.001000,loss:6.49862,auc:0.76237,lossTst:5.25171,aucTst:0.75419,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:08 Ori_fileTrends-T-new_process, Epoch0,Step700,lr:0.001000,loss:6.52709,auc:0.75414,lossTst:5.22083,aucTst:0.73723,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:14 Ori_fileTrends-T-new_process, Epoch0,Step720,lr:0.001000,loss:6.49664,auc:0.76072,lossTst:5.13888,aucTst:0.76331,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:20 Ori_fileTrends-T-new_process, Epoch0,Step740,lr:0.001000,loss:6.46584,auc:0.76830,lossTst:5.15427,aucTst:0.75579,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:26 Ori_fileTrends-T-new_process, Epoch0,Step760,lr:0.001000,loss:6.52128,auc:0.75533,lossTst:4.86624,aucTst:0.81878,MaxaucTst:0.79842,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:34 Ori_fileTrends-T-new_process, Epoch0,Step780,lr:0.001000,loss:6.48372,auc:0.76328,lossTst:5.05971,aucTst:0.78378,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:40 Ori_fileTrends-T-new_process, Epoch0,Step800,lr:0.001000,loss:6.46232,auc:0.76768,lossTst:5.17356,aucTst:0.75408,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:46 Ori_fileTrends-T-new_process, Epoch0,Step820,lr:0.001000,loss:6.41741,auc:0.77951,lossTst:4.95493,aucTst:0.81638,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:58:54 Ori_fileTrends-T-new_process, Epoch0,Step840,lr:0.001000,loss:6.39404,auc:0.78293,lossTst:5.07117,aucTst:0.76642,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:01 Ori_fileTrends-T-new_process, Epoch0,Step860,lr:0.001000,loss:6.39299,auc:0.78522,lossTst:5.24703,aucTst:0.73006,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:09 Ori_fileTrends-T-new_process, Epoch0,Step880,lr:0.001000,loss:6.38969,auc:0.78498,lossTst:4.89068,aucTst:0.81482,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:16 Ori_fileTrends-T-new_process, Epoch0,Step900,lr:0.001000,loss:6.39571,auc:0.78427,lossTst:5.09548,aucTst:0.77675,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:23 Ori_fileTrends-T-new_process, Epoch0,Step920,lr:0.001000,loss:6.39227,auc:0.78847,lossTst:5.26092,aucTst:0.74885,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:29 Ori_fileTrends-T-new_process, Epoch0,Step940,lr:0.001000,loss:6.43734,auc:0.78080,lossTst:4.83847,aucTst:0.81281,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:36 Ori_fileTrends-T-new_process, Epoch0,Step960,lr:0.001000,loss:6.42488,auc:0.77885,lossTst:5.31402,aucTst:0.71105,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:42 Ori_fileTrends-T-new_process, Epoch0,Step980,lr:0.001000,loss:6.45892,auc:0.77164,lossTst:5.11062,aucTst:0.75940,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 22:59:49 Ori_fileTrends-T-new_process, Epoch0,Step1000,lr:0.001000,loss:6.46121,auc:0.77004,lossTst:4.99234,aucTst:0.79018,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-06 22:59:55 Ori_fileTrends-T-new_process, Epoch0,Step1020,lr:0.000960,loss:6.42125,auc:0.77549,lossTst:5.09627,aucTst:0.76820,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:02 Ori_fileTrends-T-new_process, Epoch0,Step1040,lr:0.000960,loss:6.37638,auc:0.78516,lossTst:5.01000,aucTst:0.76903,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:09 Ori_fileTrends-T-new_process, Epoch0,Step1060,lr:0.000960,loss:6.42098,auc:0.77711,lossTst:4.80658,aucTst:0.81945,MaxaucTst:0.81878,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:18 Ori_fileTrends-T-new_process, Epoch0,Step1080,lr:0.000960,loss:6.39127,auc:0.77892,lossTst:5.26955,aucTst:0.74240,MaxaucTst:0.81945,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:25 Ori_fileTrends-T-new_process, Epoch0,Step1100,lr:0.000960,loss:6.38263,auc:0.78250,lossTst:4.96837,aucTst:0.77031,MaxaucTst:0.81945,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:31 Ori_fileTrends-T-new_process, Epoch0,Step1120,lr:0.000960,loss:6.34383,auc:0.78744,lossTst:4.65420,aucTst:0.85070,MaxaucTst:0.81945,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:40 Ori_fileTrends-T-new_process, Epoch0,Step1140,lr:0.000960,loss:6.37614,auc:0.77934,lossTst:5.21906,aucTst:0.74828,MaxaucTst:0.85070,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:47 Ori_fileTrends-T-new_process, Epoch0,Step1160,lr:0.000960,loss:6.37648,auc:0.78143,lossTst:5.33946,aucTst:0.69105,MaxaucTst:0.85070,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:00:53 Ori_fileTrends-T-new_process, Epoch0,Step1180,lr:0.000960,loss:6.36621,auc:0.78318,lossTst:4.95996,aucTst:0.80666,MaxaucTst:0.85070,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:00 Ori_fileTrends-T-new_process, Epoch0,Step1200,lr:0.000960,loss:6.04901,auc:0.84262,lossTst:4.46651,aucTst:0.86134,MaxaucTst:0.85070,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:09 Ori_fileTrends-T-new_process, Epoch0,Step1220,lr:0.000960,loss:5.62024,auc:0.89295,lossTst:4.28587,aucTst:0.89490,MaxaucTst:0.86134,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:18 Ori_fileTrends-T-new_process, Epoch0,Step1240,lr:0.000960,loss:5.57812,auc:0.89673,lossTst:4.00639,aucTst:0.91788,MaxaucTst:0.89490,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:27 Ori_fileTrends-T-new_process, Epoch0,Step1260,lr:0.000960,loss:5.55379,auc:0.89942,lossTst:4.12010,aucTst:0.90917,MaxaucTst:0.91788,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:34 Ori_fileTrends-T-new_process, Epoch0,Step1280,lr:0.000960,loss:5.46358,auc:0.91170,lossTst:4.21173,aucTst:0.89370,MaxaucTst:0.91788,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:40 Ori_fileTrends-T-new_process, Epoch0,Step1300,lr:0.000960,loss:5.44780,auc:0.91275,lossTst:4.44054,aucTst:0.87464,MaxaucTst:0.91788,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:47 Ori_fileTrends-T-new_process, Epoch0,Step1320,lr:0.000960,loss:5.44586,auc:0.91407,lossTst:3.95476,aucTst:0.92586,MaxaucTst:0.91788,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:01:56 Ori_fileTrends-T-new_process, Epoch0,Step1340,lr:0.000960,loss:5.36384,auc:0.92075,lossTst:4.01794,aucTst:0.92458,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:02 Ori_fileTrends-T-new_process, Epoch0,Step1360,lr:0.000960,loss:5.39149,auc:0.91759,lossTst:4.15273,aucTst:0.91338,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:09 Ori_fileTrends-T-new_process, Epoch0,Step1380,lr:0.000960,loss:5.42668,auc:0.91417,lossTst:4.03295,aucTst:0.91667,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:15 Ori_fileTrends-T-new_process, Epoch0,Step1400,lr:0.000960,loss:5.38911,auc:0.91964,lossTst:4.10413,aucTst:0.91662,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:22 Ori_fileTrends-T-new_process, Epoch0,Step1420,lr:0.000960,loss:5.35148,auc:0.92256,lossTst:4.29941,aucTst:0.89630,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:28 Ori_fileTrends-T-new_process, Epoch0,Step1440,lr:0.000960,loss:5.46261,auc:0.91058,lossTst:4.01581,aucTst:0.92183,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:35 Ori_fileTrends-T-new_process, Epoch0,Step1460,lr:0.000960,loss:5.96828,auc:0.83916,lossTst:5.16312,aucTst:0.73865,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:41 Ori_fileTrends-T-new_process, Epoch0,Step1480,lr:0.000960,loss:6.35872,auc:0.77849,lossTst:5.14620,aucTst:0.75468,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:48 Ori_fileTrends-T-new_process, Epoch0,Step1500,lr:0.000960,loss:6.39339,auc:0.77342,lossTst:4.86947,aucTst:0.80022,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:02:55 Ori_fileTrends-T-new_process, Epoch0,Step1520,lr:0.000960,loss:6.35094,auc:0.78304,lossTst:4.83984,aucTst:0.81912,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:01 Ori_fileTrends-T-new_process, Epoch0,Step1540,lr:0.000960,loss:6.30810,auc:0.78919,lossTst:4.91419,aucTst:0.78355,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:08 Ori_fileTrends-T-new_process, Epoch0,Step1560,lr:0.000960,loss:6.35616,auc:0.78191,lossTst:5.15523,aucTst:0.72920,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:17 Ori_fileTrends-T-new_process, Epoch0,Step1580,lr:0.000960,loss:6.28899,auc:0.79469,lossTst:5.16148,aucTst:0.74973,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:24 Ori_fileTrends-T-new_process, Epoch0,Step1600,lr:0.000960,loss:6.43272,auc:0.77493,lossTst:5.19894,aucTst:0.74074,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:30 Ori_fileTrends-T-new_process, Epoch0,Step1620,lr:0.000960,loss:6.45872,auc:0.77240,lossTst:5.06711,aucTst:0.77099,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:37 Ori_fileTrends-T-new_process, Epoch0,Step1640,lr:0.000960,loss:6.40193,auc:0.78035,lossTst:5.02369,aucTst:0.75882,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:44 Ori_fileTrends-T-new_process, Epoch0,Step1660,lr:0.000960,loss:6.41367,auc:0.78047,lossTst:5.27859,aucTst:0.73923,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:50 Ori_fileTrends-T-new_process, Epoch0,Step1680,lr:0.000960,loss:6.42370,auc:0.77540,lossTst:5.07199,aucTst:0.76212,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:03:56 Ori_fileTrends-T-new_process, Epoch0,Step1700,lr:0.000960,loss:6.36518,auc:0.78513,lossTst:4.98312,aucTst:0.78324,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:02 Ori_fileTrends-T-new_process, Epoch0,Step1720,lr:0.000960,loss:6.34137,auc:0.79288,lossTst:5.07261,aucTst:0.76819,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:09 Ori_fileTrends-T-new_process, Epoch0,Step1740,lr:0.000960,loss:6.35144,auc:0.79106,lossTst:5.11249,aucTst:0.75358,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:15 Ori_fileTrends-T-new_process, Epoch0,Step1760,lr:0.000960,loss:6.33581,auc:0.79345,lossTst:4.97082,aucTst:0.79812,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:21 Ori_fileTrends-T-new_process, Epoch0,Step1780,lr:0.000960,loss:6.33727,auc:0.79258,lossTst:5.07378,aucTst:0.76656,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:27 Ori_fileTrends-T-new_process, Epoch0,Step1800,lr:0.000960,loss:6.33759,auc:0.79061,lossTst:4.79802,aucTst:0.82489,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:34 Ori_fileTrends-T-new_process, Epoch0,Step1820,lr:0.000960,loss:6.37895,auc:0.78247,lossTst:5.08721,aucTst:0.75766,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:41 Ori_fileTrends-T-new_process, Epoch0,Step1840,lr:0.000960,loss:6.36629,auc:0.78223,lossTst:4.73832,aucTst:0.83815,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:48 Ori_fileTrends-T-new_process, Epoch0,Step1860,lr:0.000960,loss:6.34284,auc:0.78767,lossTst:5.03320,aucTst:0.79524,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:04:54 Ori_fileTrends-T-new_process, Epoch0,Step1880,lr:0.000960,loss:6.34765,auc:0.78732,lossTst:5.09406,aucTst:0.77366,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:00 Ori_fileTrends-T-new_process, Epoch0,Step1900,lr:0.000960,loss:6.30902,auc:0.79254,lossTst:4.96684,aucTst:0.77938,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:07 Ori_fileTrends-T-new_process, Epoch0,Step1920,lr:0.000960,loss:6.33907,auc:0.78814,lossTst:4.79646,aucTst:0.83725,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:13 Ori_fileTrends-T-new_process, Epoch0,Step1940,lr:0.000960,loss:6.22789,auc:0.80587,lossTst:4.99047,aucTst:0.79419,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:19 Ori_fileTrends-T-new_process, Epoch0,Step1960,lr:0.000960,loss:6.22165,auc:0.80855,lossTst:4.86799,aucTst:0.81034,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:26 Ori_fileTrends-T-new_process, Epoch0,Step1980,lr:0.000960,loss:6.20247,auc:0.81134,lossTst:4.81690,aucTst:0.81152,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:33 Ori_fileTrends-T-new_process, Epoch0,Step2000,lr:0.000960,loss:6.22397,auc:0.80584,lossTst:4.94626,aucTst:0.77000,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-06 23:05:39 Ori_fileTrends-T-new_process, Epoch0,Step2020,lr:0.000922,loss:6.20250,auc:0.80951,lossTst:4.84255,aucTst:0.81969,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:46 Ori_fileTrends-T-new_process, Epoch0,Step2040,lr:0.000922,loss:6.18252,auc:0.81378,lossTst:4.89456,aucTst:0.78982,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:52 Ori_fileTrends-T-new_process, Epoch0,Step2060,lr:0.000922,loss:6.14112,auc:0.82023,lossTst:4.95781,aucTst:0.78854,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:05:58 Ori_fileTrends-T-new_process, Epoch0,Step2080,lr:0.000922,loss:6.21077,auc:0.80887,lossTst:4.92765,aucTst:0.79856,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:04 Ori_fileTrends-T-new_process, Epoch0,Step2100,lr:0.000922,loss:6.17794,auc:0.82071,lossTst:4.73088,aucTst:0.82803,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:11 Ori_fileTrends-T-new_process, Epoch0,Step2120,lr:0.000922,loss:6.21422,auc:0.80989,lossTst:4.84695,aucTst:0.81087,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:17 Ori_fileTrends-T-new_process, Epoch0,Step2140,lr:0.000922,loss:6.20268,auc:0.80501,lossTst:4.92319,aucTst:0.78049,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:23 Ori_fileTrends-T-new_process, Epoch0,Step2160,lr:0.000922,loss:6.18642,auc:0.81291,lossTst:5.01550,aucTst:0.78421,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:28 Ori_fileTrends-T-new_process, Epoch0,Step2180,lr:0.000922,loss:6.18599,auc:0.80967,lossTst:4.91211,aucTst:0.81009,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:34 Ori_fileTrends-T-new_process, Epoch0,Step2200,lr:0.000922,loss:6.20356,auc:0.80645,lossTst:4.65578,aucTst:0.84808,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:40 Ori_fileTrends-T-new_process, Epoch0,Step2220,lr:0.000922,loss:6.24149,auc:0.80310,lossTst:4.40451,aucTst:0.87129,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:46 Ori_fileTrends-T-new_process, Epoch0,Step2240,lr:0.000922,loss:6.16173,auc:0.81262,lossTst:5.04727,aucTst:0.76860,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:51 Ori_fileTrends-T-new_process, Epoch0,Step2260,lr:0.000922,loss:6.11655,auc:0.82111,lossTst:5.17252,aucTst:0.74579,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:06:56 Ori_fileTrends-T-new_process, Epoch0,Step2280,lr:0.000922,loss:6.14119,auc:0.81708,lossTst:4.92220,aucTst:0.80931,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:01 Ori_fileTrends-T-new_process, Epoch0,Step2300,lr:0.000922,loss:6.14495,auc:0.81744,lossTst:4.85465,aucTst:0.80282,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:07 Ori_fileTrends-T-new_process, Epoch0,Step2320,lr:0.000922,loss:6.13270,auc:0.81659,lossTst:4.87355,aucTst:0.81363,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:12 Ori_fileTrends-T-new_process, Epoch0,Step2340,lr:0.000922,loss:6.07234,auc:0.82524,lossTst:4.92655,aucTst:0.80381,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:17 Ori_fileTrends-T-new_process, Epoch0,Step2360,lr:0.000922,loss:5.99917,auc:0.83793,lossTst:4.53053,aucTst:0.84806,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:22 Ori_fileTrends-T-new_process, Epoch0,Step2380,lr:0.000922,loss:5.88643,auc:0.85123,lossTst:4.48795,aucTst:0.86655,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:27 Ori_fileTrends-T-new_process, Epoch0,Step2400,lr:0.000922,loss:5.93361,auc:0.84718,lossTst:4.89155,aucTst:0.79144,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:32 Ori_fileTrends-T-new_process, Epoch0,Step2420,lr:0.000922,loss:6.01450,auc:0.83522,lossTst:4.72459,aucTst:0.82327,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:36 Ori_fileTrends-T-new_process, Epoch0,Step2440,lr:0.000922,loss:5.93798,auc:0.84397,lossTst:4.88639,aucTst:0.80795,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:41 Ori_fileTrends-T-new_process, Epoch0,Step2460,lr:0.000922,loss:5.91463,auc:0.84792,lossTst:4.68020,aucTst:0.83327,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:46 Ori_fileTrends-T-new_process, Epoch0,Step2480,lr:0.000922,loss:6.06657,auc:0.82669,lossTst:4.64268,aucTst:0.83242,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:51 Ori_fileTrends-T-new_process, Epoch0,Step2500,lr:0.000922,loss:5.97940,auc:0.83956,lossTst:4.49864,aucTst:0.85264,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:07:56 Ori_fileTrends-T-new_process, Epoch0,Step2520,lr:0.000922,loss:6.00797,auc:0.83562,lossTst:4.93067,aucTst:0.77623,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:01 Ori_fileTrends-T-new_process, Epoch0,Step2540,lr:0.000922,loss:6.05063,auc:0.82591,lossTst:4.79044,aucTst:0.83324,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:08 Ori_fileTrends-T-new_process, Epoch0,Step2560,lr:0.000922,loss:5.98234,auc:0.83745,lossTst:4.88687,aucTst:0.80367,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:13 Ori_fileTrends-T-new_process, Epoch0,Step2580,lr:0.000922,loss:5.94612,auc:0.83873,lossTst:4.80309,aucTst:0.79828,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:19 Ori_fileTrends-T-new_process, Epoch0,Step2600,lr:0.000922,loss:5.99440,auc:0.83503,lossTst:4.80001,aucTst:0.80941,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:25 Ori_fileTrends-T-new_process, Epoch0,Step2620,lr:0.000922,loss:5.86378,auc:0.84516,lossTst:4.74858,aucTst:0.81457,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:30 Ori_fileTrends-T-new_process, Epoch0,Step2640,lr:0.000922,loss:5.94766,auc:0.83673,lossTst:4.53982,aucTst:0.85131,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:36 Ori_fileTrends-T-new_process, Epoch0,Step2660,lr:0.000922,loss:5.93923,auc:0.83620,lossTst:4.49819,aucTst:0.86592,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:41 Ori_fileTrends-T-new_process, Epoch0,Step2680,lr:0.000922,loss:5.99107,auc:0.83300,lossTst:4.92900,aucTst:0.79751,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:46 Ori_fileTrends-T-new_process, Epoch0,Step2700,lr:0.000922,loss:6.04225,auc:0.82545,lossTst:4.72013,aucTst:0.82563,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:52 Ori_fileTrends-T-new_process, Epoch0,Step2720,lr:0.000922,loss:6.13054,auc:0.81514,lossTst:4.82857,aucTst:0.81628,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:08:57 Ori_fileTrends-T-new_process, Epoch0,Step2740,lr:0.000922,loss:6.11595,auc:0.81737,lossTst:4.80725,aucTst:0.82353,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:03 Ori_fileTrends-T-new_process, Epoch0,Step2760,lr:0.000922,loss:6.08451,auc:0.82312,lossTst:4.76564,aucTst:0.80669,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:08 Ori_fileTrends-T-new_process, Epoch0,Step2780,lr:0.000922,loss:6.12828,auc:0.82057,lossTst:4.93357,aucTst:0.80687,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:14 Ori_fileTrends-T-new_process, Epoch0,Step2800,lr:0.000922,loss:6.04637,auc:0.82767,lossTst:4.47915,aucTst:0.84742,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:19 Ori_fileTrends-T-new_process, Epoch0,Step2820,lr:0.000922,loss:6.07328,auc:0.82795,lossTst:4.75180,aucTst:0.82195,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:24 Ori_fileTrends-T-new_process, Epoch0,Step2840,lr:0.000922,loss:6.04060,auc:0.82997,lossTst:4.86633,aucTst:0.81413,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:30 Ori_fileTrends-T-new_process, Epoch0,Step2860,lr:0.000922,loss:6.14196,auc:0.81934,lossTst:4.76009,aucTst:0.81962,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n",
      "2023-01-06 23:09:35 Ori_fileTrends-T-new_process, Epoch0,Step2880,lr:0.000922,loss:6.06487,auc:0.82712,lossTst:4.85849,aucTst:0.80027,MaxaucTst:0.92586,tolerant:40.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert.n_epochs = 1\n",
    "bert.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [str(int(float(i[0]))) for i in list(bert.session_cluster_matrix)]\n",
    "X = [str(i) for i in list(np.arange(len(Y)))]\n",
    "Y = Y[:-1]\n",
    "X = X[:-1]\n",
    "doc_emb = bert.session_embedding_matrix[:-1]\n",
    "\n",
    "#doc_emb = normalize(doc_emb, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199141, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_label = [int(float(i)) for i in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.8445835158008371\n",
      "0.2   0.8461124316274807\n",
      "0.3   0.8462394813529354\n",
      "0.4   0.8465635148042024\n",
      "0.5   0.8468895572754283\n",
      "0.6   0.8460497804478176\n",
      "0.7   0.8477088331015936\n",
      "0.8   0.8483011097128474\n",
      "0.9   0.8448882592203282\n",
      "0.8463707203714966\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['micro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['micro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.8320059229631067\n",
      "0.2   0.8336770716744255\n",
      "0.3   0.8339943892292763\n",
      "0.4   0.8344000754073383\n",
      "0.5   0.8347048558203765\n",
      "0.6   0.833972711477023\n",
      "0.7   0.8350601220549807\n",
      "0.8   0.835334896267313\n",
      "0.9   0.8316541888910122\n",
      "0.8338671370872057\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['macro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['macro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.8362244527889213\n",
      "0.2   0.8379416620112609\n",
      "0.3   0.8381767444529731\n",
      "0.4   0.8384232330418044\n",
      "0.5   0.838577497464121\n",
      "0.6   0.8379677868862749\n",
      "0.7   0.8397301775940278\n",
      "0.8   0.8402420346983354\n",
      "0.9   0.8367060005021341\n",
      "0.8382210654933169\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
