{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import model\n",
    "import random\n",
    "\n",
    "class Args():\n",
    "    is_training = False\n",
    "    layers = 1\n",
    "    rnn_size = 100  # RNN hidden unit num\n",
    "    n_epochs = 1  # number of epochs\n",
    "    batch_size = 50  # default 50\n",
    "    dropout_p_hidden = 1  # dropout probability in hidden layer\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.96  # used for batch norm\n",
    "    decay_steps = 1000\n",
    "    sigma = 0  # used for variable initial\n",
    "    init_as_normal = False\n",
    "    reset_after_session = True  # wheather resetting the hidden state when a session is finished\n",
    "    session_key = 'SessionId'  # column 1\n",
    "    item_key = 'ItemId'  # column 2\n",
    "    time_key = 'Time'  # column 3\n",
    "    grad_cap = 0\n",
    "    test_model = 1  # test model version\n",
    "    base_path = ''\n",
    "    checkpoint_dir = ''  # directory of check point\n",
    "    model_name = ''\n",
    "    loss = 'cross-entropy'\n",
    "    #loss = 'bpr'\n",
    "    final_act = 'softmax'  # activation function of the final layer\n",
    "    hidden_act = 'tanh'  # activation function of the hidden layer\n",
    "    n_items = -1  # number of itemes\n",
    "    ori_file = ''  # ds of data\n",
    "    rnn_type = ''\n",
    "    is_preprocess = 0\n",
    "    bert_layer = 2\n",
    "    head_num = 2\n",
    "    predict_sequence_length = 50\n",
    "    \n",
    "    attn_item_sbj_hidden = 8\n",
    "    \n",
    "    ## keep number of neighbors\n",
    "    max_degree = 100\n",
    "    enrich_seq_num = 2\n",
    "\n",
    "\n",
    "def parseArgs():\n",
    "    parser = argparse.ArgumentParser(description='CGraphNet args')\n",
    "    parser.add_argument('--layer', default=1, type=int)  # default for single layer GRU\n",
    "    parser.add_argument('--size', default=256, type=int)  # rnn size(dimension)\n",
    "    parser.add_argument('--epoch', default=2, type=int)\n",
    "    parser.add_argument('--batch_size', default=128, type=int)\n",
    "    parser.add_argument('--lr', default=0.001, type=float)  # learning rate\n",
    "    parser.add_argument('--train', default=1, type=int)\n",
    "    parser.add_argument('--test', default=1, type=int)\n",
    "    parser.add_argument('--hidden_act', default='tanh', type=str)\n",
    "    parser.add_argument('--final_act', default='softmax', type=str)\n",
    "    parser.add_argument('--loss', default='cross-entropy', type=str)\n",
    "    #parser.add_argument('--loss', default='bpr', type=str)\n",
    "    parser.add_argument('--dropout', default='1.0', type=float)\n",
    "    parser.add_argument('--test_model', default=0, type=int)\n",
    "    parser.add_argument('--base_path', default='', type=str)\n",
    "    parser.add_argument('--ori_file', default='', type=str)\n",
    "    parser.add_argument('--model_name', default='', type=str)\n",
    "    parser.add_argument('--rnn_type', default='gru', type=str)\n",
    "    parser.add_argument('--decay_steps', default=1000, type=int)\n",
    "    parser.add_argument('--is_preprocess', default=0, type=int)\n",
    "    parser.add_argument('--predict_sequence_length', default= 20, type=int)\n",
    "    parser.add_argument('--bert_layer', default=1, type=int)\n",
    "    parser.add_argument('--head_num', default=1, type=int)\n",
    "    parser.add_argument('--attn_item_sbj_hidden', default=8, type=int)\n",
    "    parser.add_argument('--sliding_window', default=3, type=int)\n",
    "    parser.add_argument('--max_degree', default=10, type=int)\n",
    "    parser.add_argument('--enrich_seq_num', default=2, type=int)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['foo']\n",
    "command_line = parseArgs()       # get external parameters\n",
    "args = Args()\n",
    "\n",
    "args.layers = command_line.layer\n",
    "args.rnn_size = command_line.size\n",
    "args.n_epochs = command_line.epoch\n",
    "args.learning_rate = command_line.lr\n",
    "args.is_training = command_line.train\n",
    "args.test_model = command_line.test\n",
    "args.batch_size = command_line.batch_size\n",
    "args.hidden_act = command_line.hidden_act\n",
    "args.final_act = command_line.final_act\n",
    "args.loss = command_line.loss\n",
    "args.test_model = command_line.epoch - 1  # use the model saved by the last epoch\n",
    "args.dropout_p_hidden = 1.0 if args.is_training == 0 else command_line.dropout  # use dropout(0.5) in training phase\n",
    "args.base_path = command_line.base_path\n",
    "args.ori_file = command_line.ori_file\n",
    "args.checkpoint_dir = args.base_path + 'checkpoint_CGraphNet/' + args.ori_file\n",
    "args.model_name = command_line.model_name\n",
    "args.rnn_type = command_line.rnn_type\n",
    "args.decay_steps = command_line.decay_steps\n",
    "args.is_preprocess = command_line.is_preprocess\n",
    "args.predict_sequence_length = command_line.predict_sequence_length\n",
    "args.bert_layer = command_line.bert_layer\n",
    "args.head_num = command_line.head_num\n",
    "\n",
    "args.sliding_window = command_line.sliding_window\n",
    "\n",
    "args.savedModel = 1\n",
    "\n",
    "args.attn_item_sbj_hidden = command_line.attn_item_sbj_hidden\n",
    "args.max_degree = command_line.max_degree\n",
    "args.enrich_seq_num = command_line.enrich_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.base_path = \"short_text_data/\"\n",
    "args.ori_file = \"Tweet_process\"\n",
    "#2020052812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.checkpoint_dir):\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "        with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('mkdir ' + args.checkpoint_dir)\n",
    "else:\n",
    "    with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "        log_file.write('already exists: ' + args.checkpoint_dir)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"     # specify which GPU(s) to be used\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[int(x)] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['accuracy'] = accuracy_score(Y, Y_)\n",
    "        \n",
    "        # print('Results, using embeddings of dimensionality', len(self.embeddings[X[0]]))\n",
    "        # print('-------------------')\n",
    "        #print(results)\n",
    "        return results\n",
    "        # print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "    \n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import layers\n",
    "import random\n",
    "# from utils import *\n",
    "# from rnn import *\n",
    "import networkx as nx\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class BERT4Rec:\n",
    "    def __init__(self, sess, args):\n",
    "        self.sess = sess\n",
    "        self.is_preprocess = args.is_preprocess\n",
    "\n",
    "        self.is_training = args.is_training\n",
    "        self.layers = args.layers\n",
    "        self.rnn_size = args.rnn_size\n",
    "        self.n_epochs = args.n_epochs\n",
    "\n",
    "        self.dropout_p_hidden = args.dropout_p_hidden\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.decay = args.decay  # default:0.96\n",
    "        self.decay_steps = args.decay_steps  # default:1e4\n",
    "        self.sigma = args.sigma  # param for initializer, default:0\n",
    "        self.init_as_normal = args.init_as_normal  # default:False\n",
    "        self.reset_after_session = args.reset_after_session  # default:True\n",
    "        self.session_key = args.session_key\n",
    "        self.item_key = args.item_key\n",
    "        self.time_key = args.time_key\n",
    "        self.grad_cap = args.grad_cap  # default:0\n",
    "        self.base_path = args.base_path\n",
    "        self.model_name = args.model_name\n",
    "        self.rnn_type = args.rnn_type\n",
    "        self.ori_file = args.ori_file\n",
    "        self.test_model = args.test_model\n",
    "        self.sequence_length = args.predict_sequence_length\n",
    "        self.max_click = 200\n",
    "\n",
    "        self.bert_layer = args.bert_layer\n",
    "        self.head_num = args.head_num\n",
    "        self.um_tag_length = 30\n",
    "        self.um_sbj_length = 30\n",
    "\n",
    "        self.train_path = self.base_path\n",
    "        self.batch_size_pos = args.batch_size\n",
    "        \n",
    "        self.attn_item_sbj_hidden = args.attn_item_sbj_hidden\n",
    "        \n",
    "        \n",
    "        self.max_degree = 100\n",
    "        self.tolerant_time = 40\n",
    "\n",
    "        if self.is_preprocess == 1:\n",
    "            return\n",
    "\n",
    "        self.time_emb_num = 30\n",
    "        self.short_emb_size = 4\n",
    "\n",
    "        if args.hidden_act == 'tanh':\n",
    "            self.hidden_act = self.tanh\n",
    "        elif args.hidden_act == 'relu':\n",
    "            self.hidden_act = self.relu\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # different loss function can try with different final_activation in training phase\n",
    "        if args.loss == 'cross-entropy':  # point-wise ranking loss\n",
    "            if args.final_act == 'tanh':\n",
    "                self.final_activation = self.softmaxth\n",
    "            else:\n",
    "                self.final_activation = self.softmax\n",
    "            self.loss_function = self.cross_entropy\n",
    "        elif args.loss == 'bpr':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activation = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.bpr\n",
    "        elif args.loss == 'top1':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activatin = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.top1\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\"[!] Checkpoint Dir not found\")\n",
    "\n",
    "    def init_model(self):\n",
    "\n",
    "        self.n_items = max(self.ItemIdxList) + 2\n",
    "        #self.n_tags = max(self.TagIdxList) + 2\n",
    "\n",
    "        self.predictTopN = self.n_items\n",
    "        # if max(self.ItemIdxList) > 100000:\n",
    "        #     self.predictTopN = 100000\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        print(\"init1\")\n",
    "        if self.is_training == 1:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        print(\"init2\")\n",
    "        #         self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n",
    "        self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        print(\"init3\")\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # use self.predict_state to hold hidden states during prediction.\n",
    "        #         self.predict_state = [np.zeros([self.batch_size, self.rnn_size], dtype=np.float32) for _ in range(self.layers)]\n",
    "        print(\"init4\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, '{}/bert-model-{}'.format(self.checkpoint_dir, self.test_model))\n",
    "        else:\n",
    "            print('no {}/bert-model-{} !!!!!'.format(self.checkpoint_dir, self.test_model))\n",
    "        print(\"init5\")\n",
    "\n",
    "    ########################ACTIVATION FUNCTIONS#########################\n",
    "    def linear(self, X):\n",
    "        return X\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return tf.nn.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        return tf.nn.softmax(X)\n",
    "\n",
    "    def softmaxth(self, X):\n",
    "        return tf.nn.softmax(tf.tanh(X))\n",
    "\n",
    "    def relu(self, X):\n",
    "        return tf.nn.relu(X)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return tf.nn.sigmoid(X)\n",
    "\n",
    "    ############################LOSS FUNCTIONS######################\n",
    "    def cross_entropy(self, yhat):  # yhat is a tensor with shape of [batch_size, batch_size]\n",
    "        return tf.reduce_mean(-tf.log(tf.diag_part(yhat) + 1e-24))\n",
    "\n",
    "    def corss_entropy_2(self, yhat):\n",
    "        return tf.reduce_mean(-tf.log(yhat[:, 0] + 1e-24))\n",
    "\n",
    "    def bpr_2(self, yhat):\n",
    "        yhat_pos = yhat[:, 0]\n",
    "        yhat_pos_column = tf.reshape(yhat_pos, [-1, 1])\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(yhat_pos_column - yhat)))\n",
    "\n",
    "    def bpr(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(tf.diag_part(yhat) - yhatT)))\n",
    "\n",
    "    def top1(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat) + yhatT) + tf.nn.sigmoid(yhatT ** 2), axis=0)\n",
    "        term2 = tf.nn.sigmoid(tf.diag_part(yhat) ** 2) / self.batch_size\n",
    "        return tf.reduce_mean(term1 - term2)\n",
    "\n",
    "    def build_transformer(self, layer_num, head_num, model_size, initializer):\n",
    "        W_TRFM = {}\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                # define qkv\n",
    "                variable_WQ = 'WQ_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WQ] = tf.get_variable(variable_WQ, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "                variable_WK = 'WK_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WK] = tf.get_variable(variable_WK, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "                variable_WV = 'WV_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WV] = tf.get_variable(variable_WV, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "\n",
    "            # define ffn variables\n",
    "            variable_WFFN = 'WFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_WFFN] = tf.get_variable(variable_WFFN, [model_size, model_size], initializer=initializer)\n",
    "\n",
    "            variable_BFFN = 'BFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_BFFN] = tf.get_variable(variable_BFFN, [1, 1, model_size], initializer=initializer)\n",
    "\n",
    "        #             # define ffn variables\n",
    "        #             variable_WFFN = 'WFFN2_layer_' + str(layer)\n",
    "        #             W_TRFM[variable_WFFN] = tf.get_variable(variable_WFFN, [model_size, model_size], initializer=initializer)\n",
    "\n",
    "        #             variable_BFFN = 'BFFN2_layer_' + str(layer)\n",
    "        #             W_TRFM[variable_BFFN] = tf.get_variable(variable_BFFN, [1, 1, model_size], initializer=initializer)\n",
    "\n",
    "        return W_TRFM\n",
    "\n",
    "    def run_transformer(self, layer_num, head_num, model_size, input_trfm, W_TRFM):\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                attn_Q = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WQ_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_K = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WK_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_V = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WV_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_out = tf.matmul(tf.nn.softmax(\n",
    "                    tf.matmul(attn_Q, tf.transpose(attn_K, perm=[0, 2, 1])) / tf.sqrt(tf.cast(model_size, tf.float32))),\n",
    "                    attn_V)\n",
    "                if head == 0:\n",
    "                    attn_concat = attn_out\n",
    "                else:\n",
    "                    attn_concat = tf.concat([attn_concat, attn_out], 2)\n",
    "\n",
    "            attn_add = attn_concat + input_trfm\n",
    "            attn_norm = tf.contrib.layers.layer_norm(attn_add, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "            attn_ffn = tf.nn.relu(tf.add(tf.tensordot(attn_norm, W_TRFM['WFFN1_layer_' + str(layer)], axes=1),\n",
    "                                         tf.tile(W_TRFM['BFFN1_layer_' + str(layer)],\n",
    "                                                 [self.batch_size, self.sequence_length, 1])))\n",
    "\n",
    "            attn_add2 = attn_norm + attn_ffn\n",
    "            attn_norm2 = tf.contrib.layers.layer_norm(attn_add2, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "\n",
    "            input_trfm = attn_norm2\n",
    "\n",
    "        return input_trfm\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        # doc id to doc index\n",
    "        self.item_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32,\n",
    "                                                                     default_value=-1, name=\"itemIdMap\",\n",
    "                                                                     checkpoint=True)\n",
    "        #self.tag_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32, default_value=-1, name=\"tagIdMap\", checkpoint=True)\n",
    "\n",
    "        # doc index to doc id\n",
    "        self.item_id_hash_table_reverse = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int64,\n",
    "                                                                             default_value=-1, name=\"itemIdMap\",\n",
    "                                                                             checkpoint=True)\n",
    "\n",
    "        self.input_item_id_tensor = tf.placeholder(tf.int64, [None, None], name='input_item_id_tensor')  # [batch_size, sequence_length]\n",
    "        #self.input_tag_id_tensor = tf.placeholder(tf.int64, [None, None],name='input_tag_id_tensor')  # [batch_size, sequence_length]\n",
    " \n",
    "        self.X_time_diff = tf.placeholder(tf.int32, [None, None], name='input_time_diff')  # [batch_size, sequence_length]\n",
    "\n",
    "        self.target_item_id_tensor = tf.placeholder(tf.int64, [None], name='target_item_id_tensor')  # [batch_size]\n",
    "\n",
    "        self.X = self.item_id_hash_table.lookup(self.input_item_id_tensor)\n",
    "        #self.X_tag = self.tag_id_hash_table.lookup(self.input_tag_id_tensor)\n",
    "        \n",
    "        #find neigbor item X \n",
    "        self.X_neig = tf.nn.embedding_lookup(self.adj_item_item_tensor, self.X+1) \n",
    "        adj_lists = tf.reshape(self.X_neig, [-1, self.max_degree])\n",
    "        adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        adj_lists = tf.slice(adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.X_neig = tf.reshape(adj_lists, [tf.shape(self.X)[0], tf.shape(self.X)[1], self.num_samples])\n",
    "        ###\n",
    "\n",
    "        self.Y = self.item_id_hash_table.lookup(self.target_item_id_tensor)\n",
    "        #find neigbor item Y\n",
    "        self.Y_expand = tf.expand_dims(self.Y, 0)\n",
    "        self.Y_neig = tf.nn.embedding_lookup(self.adj_item_item_tensor, self.Y_expand+1) \n",
    "        target_adj_lists = tf.reshape(self.Y_neig, [-1, self.max_degree])\n",
    "        target_adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(target_adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        target_adj_lists = tf.slice(target_adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.Y_neig = tf.reshape(target_adj_lists, [tf.shape(self.Y_expand)[0], tf.shape(self.Y_expand)[1], self.num_samples])\n",
    "        ###\n",
    "        ### find random item Y\n",
    "        self.random_Y_neig = tf.nn.embedding_lookup(self.random_item_item_tensor, self.Y_expand+1) \n",
    "        random_target_adj_lists = tf.reshape(self.random_Y_neig, [-1, self.max_degree])\n",
    "        random_target_adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(random_target_adj_lists)))\n",
    "        self.num_samples = 20\n",
    "        random_target_adj_lists = tf.slice(random_target_adj_lists, [0,0], [-1, self.num_samples])\n",
    "        self.random_Y_neig = tf.reshape(random_target_adj_lists, [tf.shape(self.Y_expand)[0], tf.shape(self.Y_expand)[1], self.num_samples])\n",
    "        \n",
    "        \n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)  # global step can not be trained.\n",
    "\n",
    "        with tf.variable_scope('bert_layer'):\n",
    "            sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (self.n_items + self.rnn_size))\n",
    "            if self.init_as_normal:\n",
    "                initializer = tf.random_normal_initializer(mean=0, stddev=sigma)\n",
    "            else:\n",
    "                initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)  # for default\n",
    "\n",
    "            embedding = tf.get_variable('embedding', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            #embedding_tag = tf.get_variable('embedding_tag', [self.n_tags, self.rnn_size], initializer=initializer)\n",
    "         \n",
    "                                            \n",
    "            embedding_item_neigh_weights = tf.get_variable('item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            embedding_item_self_weights = tf.get_variable('item_self_weights', [self.rnn_size, self.rnn_size], initializer=initializer)                              \n",
    "            \n",
    "            target_embedding_item_neigh_weights = tf.get_variable('target_item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            target_embedding_item_self_weights = tf.get_variable('target_item_self_weights', [self.rnn_size, self.rnn_size], initializer=initializer)                              \n",
    "            #random_target_embedding_item_neigh_weights = tf.get_variable('random_target_embedding_item_neigh_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            \n",
    "            embedding_pos = tf.get_variable('embedding_pos', [1, self.max_click, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            feat_W = tf.get_variable('feat_w', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            feat_b = tf.get_variable('feat_b', [self.rnn_size], initializer=initializer)\n",
    "\n",
    "\n",
    "            ###\n",
    "            #user_W = tf.get_variable('user_w', [self.rnn_size * 2, self.rnn_size ], initializer=initializer)\n",
    "            user_W = tf.get_variable('user_w', [self.rnn_size * 1, self.rnn_size ], initializer=initializer)\n",
    "            user_b = tf.get_variable('user_b', [self.rnn_size ], initializer=initializer)\n",
    "\n",
    "            embedding_time_diff = tf.get_variable('embedding_time_diff', [self.time_emb_num, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            softmax_W = tf.get_variable('softmax_w', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            softmax_b = tf.get_variable('softmax_b', [self.n_items], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            self.W_TRFM = self.build_transformer(self.bert_layer, self.head_num, self.rnn_size, initializer)\n",
    "\n",
    "\n",
    "            ### input process\n",
    "            self.inputs_item = tf.nn.embedding_lookup(embedding, self.X + 1)  # [batch_size, sequence_length, rnn_size]\n",
    "            #add item neighor aggregator\n",
    "            #self.inputs_item_tmp = tf.nn.leaky_relu(tf.tensordot(self.inputs_item, embedding_item_self_weights, axes=1), alpha=0.2)\n",
    "            self.inputs_item_tmp = tf.tensordot(self.inputs_item, embedding_item_self_weights, axes=1)\n",
    "            self.inputs_item_tmp = tf.expand_dims(self.inputs_item_tmp, 2) # [batch_size, sequence_length, 1, rnn_size]\n",
    "            \n",
    "            self.inputs_item_neig = tf.nn.embedding_lookup(embedding, self.X_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            #self.inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.inputs_item_neig, embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            self.inputs_item_neig = tf.tensordot(self.inputs_item_neig, embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            self.inputs_item_attn = tf.squeeze(tf.matmul(tf.nn.softmax(tf.matmul(self.inputs_item_tmp, tf.transpose(self.inputs_item_neig, perm=[0, 1, 3, 2])) / tf.sqrt(tf.cast(self.rnn_size, tf.float32))), self.inputs_item_neig), [2])\n",
    "            \n",
    "            self.inputs_item =  tf.add_n([self.inputs_item, self.inputs_item_attn])\n",
    "            ###\n",
    "            \n",
    "            #inputs_tag = tf.nn.embedding_lookup(embedding_tag, self.X_tag + 1)  #\n",
    "\n",
    "            # [batch_size, sequence_length, rnn_size * 2]\n",
    "            #inputs_feat = tf.concat([self.inputs_item, inputs_tag], 2)\n",
    "            inputs_feat = self.inputs_item\n",
    "\n",
    "            # [batch_size, sequence_length, rnn_size]\n",
    "            self.inputs_time_diff = tf.nn.embedding_lookup(embedding_time_diff, self.X_time_diff + 1)\n",
    "            \n",
    "            ###\n",
    "            self.inputs_tmp = tf.nn.tanh( tf.add(tf.tensordot(inputs_feat, feat_W, axes=1), feat_b)) + self.inputs_time_diff\n",
    "            \n",
    "            #self.inputs_tmp = tf.nn.tanh( tf.add(tf.tensordot(inputs_feat, feat_W, axes=1), feat_b)) \n",
    "\n",
    "            self.inputs = tf.reshape(self.inputs_tmp, [self.batch_size, -1, self.rnn_size])\n",
    "\n",
    "            embedding_pos_slice = tf.slice(embedding_pos, [0, 0, 0], [1, self.sequence_length, self.rnn_size])\n",
    "            embedding_pos_rev = tf.reverse(embedding_pos_slice, [1])\n",
    "\n",
    "            self.inputs_pe = tf.add(self.inputs, tf.tile(embedding_pos_rev, [self.batch_size, 1, 1]))\n",
    "\n",
    "            self.trfm_input_um = self.inputs_pe\n",
    "\n",
    "            self.trfm_out  = self.run_transformer(self.bert_layer, self.head_num, self.rnn_size, self.trfm_input_um, self.W_TRFM)\n",
    "            \n",
    "            self.trfm_out   = tf.nn.dropout(self.trfm_out, keep_prob=self.dropout_p_hidden)\n",
    "            \n",
    "            ###\n",
    "            #self.trfm_final = tf.slice(self.trfm_out, [0, self.sequence_length - 2, 0], [self.batch_size, 2, self.rnn_size])\n",
    "            self.trfm_final = tf.slice(self.trfm_out, [0, self.sequence_length - 1, 0], [self.batch_size, 1, self.rnn_size])\n",
    "            self.final_state = tf.reshape(self.trfm_final, [self.batch_size, -1])\n",
    "            # self.final_state = tf.squeeze(self.trfm_final)\n",
    "\n",
    "        self.user_vec1 = tf.reshape(self.final_state, [self.batch_size, -1])\n",
    "        \n",
    "        self.user_final = tf.nn.leaky_relu(tf.add(tf.matmul(self.user_vec1, user_W), user_b))\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            #sampled_W = tf.nn.embedding_lookup(softmax_W, self.Y + 1)\n",
    "            #sampled_b = tf.nn.embedding_lookup(softmax_b, self.Y + 1)\n",
    "            sampled_W = tf.nn.embedding_lookup(embedding, self.Y + 1)\n",
    "            sampled_b = tf.nn.embedding_lookup(softmax_b, self.Y + 1)\n",
    "            \n",
    "            ### target process\n",
    "            self.Y_expand = tf.expand_dims(self.Y, 0)\n",
    "            self.target_inputs_item = tf.nn.embedding_lookup(embedding, self.Y_expand + 1)  # [batch_size, sequence_length, rnn_size]\n",
    "            self.target_inputs_item_tmp = tf.nn.leaky_relu(tf.tensordot(self.target_inputs_item, target_embedding_item_self_weights, axes=1), alpha=0.2)\n",
    "            #self.target_inputs_item_tmp = tf.tensordot(self.target_inputs_item, target_embedding_item_self_weights, axes=1)\n",
    "            self.target_inputs_item_tmp = tf.expand_dims(self.target_inputs_item_tmp, 2) # [batch_size, sequence_length, 1, rnn_size]\n",
    "            \n",
    "            \n",
    "            self.target_inputs_item_neig = tf.nn.embedding_lookup(embedding, self.Y_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            self.target_inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            #self.target_inputs_item_neig = tf.tensordot(self.target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            self.random_target_inputs_item_neig = tf.nn.embedding_lookup(embedding, self.random_Y_neig + 1)  # [batch_size, sequence_length, num_samples, rnn_size]\n",
    "            self.random_target_inputs_item_neig = tf.nn.leaky_relu(tf.tensordot(self.random_target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1), alpha=0.2)\n",
    "            #self.random_target_inputs_item_neig = tf.tensordot(self.random_target_inputs_item_neig, target_embedding_item_neigh_weights, axes=1)\n",
    "            \n",
    "            #self.target_inputs_item_attn = tf.squeeze(tf.matmul(tf.nn.softmax(tf.matmul(self.target_inputs_item_tmp, tf.transpose(self.target_inputs_item_neig, perm=[0, 1, 3, 2])) / tf.sqrt(tf.cast(self.rnn_size, tf.float32))), self.target_inputs_item_neig), [2])\n",
    "            self.target_inputs_item_attn = tf.reduce_sum(self.target_inputs_item_neig,[2])\n",
    "            self.random_target_inputs_item_attn = tf.reduce_sum(self.random_target_inputs_item_neig,[2])\n",
    "            \n",
    "            #self.target_inputs_item =  tf.add_n([self.target_inputs_item, self.target_inputs_item_attn])\n",
    "            self.target_inputs_item =  self.target_inputs_item_attn\n",
    "            \n",
    "            ###\n",
    "            #self.target_inputs_item = tf.squeeze(self.target_inputs_item, [0])\n",
    "            #sampled_W =  tf.add_n([sampled_W, self.target_inputs_item])\n",
    "            \n",
    "            self.logits = tf.matmul(self.user_final, sampled_W, transpose_b=True) + sampled_b\n",
    "            self.contrastive_logits = tf.matmul(self.user_final, self.target_inputs_item, transpose_b=True) + sampled_b\n",
    "            self.random_contrastive_logits = tf.matmul(self.user_final, self.random_target_inputs_item_attn, transpose_b=True) + sampled_b\n",
    "            \n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "            self.contrastive_yhat = tf.squeeze(self.final_activation(self.contrastive_logits), [0])\n",
    "            self.random_contrastive_yhat = tf.squeeze(self.final_activation(self.random_contrastive_logits), [0])\n",
    "            \n",
    "            self.target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            #self.target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            #self.random_target_cost = tf.reduce_mean(-tf.log(tf.exp(tf.diag_part(self.contrastive_yhat)) / (tf.exp(tf.diag_part(self.yhat)) + tf.exp(tf.diag_part(self.contrastive_yhat)) + tf.exp(tf.diag_part(self.random_contrastive_yhat))+ 1e-24) ))\n",
    "            \n",
    "            self.cost = self.loss_function(self.yhat) + self.target_cost \n",
    "            #self.cost = self.loss_function(self.yhat) \n",
    "            \n",
    "        else:\n",
    "            softmax_W_topN = tf.slice(softmax_W, [0, 0], [self.predictTopN, self.rnn_size])\n",
    "            softmax_b_topN = tf.slice(softmax_b, [0], [self.predictTopN])\n",
    "            self.logits = tf.matmul(self.user_final, softmax_W_topN, transpose_b=True) + softmax_b_topN\n",
    "            # self.logits = tf.matmul(self.user_vec2, softmax_W, transpose_b=True) + softmax_b\n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "\n",
    "            # ## savedModel ## #\n",
    "            self.yhat = tf.identity(self.yhat, name=\"yhat\")\n",
    "\n",
    "            # topNum = tf.divide(tf.constant(1000), tf.max(tf.constant(1), outSeqNum))\n",
    "            self.topk_prob_sample, self.topk_ind_sample = tf.nn.top_k(self.yhat, 4000, name=\"topk\")\n",
    "\n",
    "            self.topk_ind_sample = self.topk_ind_sample - 1\n",
    "\n",
    "            self.topk_doc_id = self.item_id_hash_table_reverse.lookup(tf.cast(self.topk_ind_sample, tf.int64))\n",
    "\n",
    "            topk_prob_sample_1d = tf.reshape(self.topk_prob_sample, [-1])\n",
    "            topk_ind_sample_1d = tf.reshape(self.topk_ind_sample, [-1])\n",
    "            topk_doc_id_1d = tf.reshape(self.topk_doc_id, [-1])\n",
    "\n",
    "            self.topk_ind_val = tf.add(topk_ind_sample_1d, tf.constant(0), name=\"topk_ind_val\")\n",
    "            self.topk_prob_val = tf.add(topk_prob_sample_1d, tf.constant(0.0), name=\"topk_prob_val\")\n",
    "            self.topk_doc_id = tf.identity(topk_doc_id_1d, name=\"topk_doc_id\")\n",
    "            return\n",
    "\n",
    "        self.lr = tf.maximum(1e-5, tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n",
    "                                                              self.decay, staircase=True))\n",
    "\n",
    "        ''' Try different optimizers. '''\n",
    "        # optimizer = tf.train.AdagradOptimizer(self.lr)\n",
    "        # optimizer = tf.train.AdadeltaOptimizer(self.lr)\n",
    "        # optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        gvs = optimizer.compute_gradients(self.cost, tvars)\n",
    "        if self.grad_cap > 0:\n",
    "            capped_gvs = [(tf.clip_by_norm(grad, self.grad_cap), var) for grad, var in gvs]\n",
    "        else:\n",
    "            capped_gvs = gvs\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs, global_step=self.global_step)\n",
    "        \n",
    "    def save_obj(self, path, obj, name):\n",
    "        with open(path + name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(self, path, name):\n",
    "        with open(path + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "            \n",
    "    def preprocess(self, verbose=False):\n",
    "\n",
    "        \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write( \"{} begin preprocessing...\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) + '\\n')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('start loading file: train_file: ' + self.train_path + self.ori_file + '\\n')\n",
    "\n",
    "        #         try:\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('loading raw data. \\n')\n",
    "            \n",
    "        mv_data_pd = pd.read_pickle(self.base_path + self.ori_file+\".pkl\")\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('finish loading raw data. \\n')\n",
    "\n",
    "        \n",
    "\n",
    "        # get expand data\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting expand data. \\n')\n",
    "\n",
    "        mv_data_pd['Sequence'] = mv_data_pd['Sequence'].astype('str')\n",
    "        \n",
    "        data_expand = pd.concat([Series(row['SessionId'],row['Sequence'].split('#')) for _, row in mv_data_pd.iterrows()]).reset_index()\n",
    "                                 \n",
    "        data_expand.columns = ['ItemInfo', 'SessionId']\n",
    "        data_expand[['ItemId', 'Time' ]] = pd.DataFrame( data_expand.ItemInfo.str.split('_').tolist())\n",
    "\n",
    "        # get item, tag, subject id-idx dictionary dataframe\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting item, tag, subject id-idx dictionary. \\n')\n",
    "\n",
    "        ItemIdCount = data_expand['ItemId'].value_counts()\n",
    "        ItemIdFreq = ItemIdCount.keys().tolist()\n",
    "        ItemIdFreqPd = pd.DataFrame(ItemIdFreq)\n",
    "        ItemIdFreqPd.columns = ['ItemId']\n",
    "        ItemIdPd = ItemIdFreqPd\n",
    "        ItemIdPd['ItemIdx'] = range(len(ItemIdFreq))\n",
    "        ItemIdPd.columns = ['ItemId', 'ItemIdx']\n",
    "\n",
    "        data_ItemInfo = data_expand[['ItemId']]\n",
    "        data_ItemInfo.drop_duplicates(inplace=True)\n",
    "        # data_ItemInfo = pd.merge(data_ItemInfo, ItemIdFreqPd, on='ItemId', how='inner')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting feature data. \\n')\n",
    "            \n",
    "            \n",
    "        # get feature data\n",
    "        data_ItemInfo.dropna(inplace=True)\n",
    "        \n",
    "        data = data_expand[['SessionId', 'ItemId', 'Time']]\n",
    "        data = pd.merge(data, data_ItemInfo, on='ItemId', how='inner')\n",
    "        \n",
    "        data = pd.merge(data, mv_data_pd[['SessionId','Cluster']], on='SessionId', how='inner')\n",
    "\n",
    "        \n",
    "        data = data.sort_values(by=['SessionId', 'Time'])\n",
    "        #data = data.sort_values(by=['SessionId'])\n",
    "        \n",
    "        data = data[data['ItemId'] != '']\n",
    "        data = data[data['Time'] != '']\n",
    "        \n",
    "        data.ItemId = data.ItemId.astype(int)\n",
    "        data.Time = data.Time.astype(int)\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting offset_sessions. \\n')\n",
    "\n",
    "        # get offset_sessions\n",
    "        offset_sessions = np.zeros(data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('saving data by pickle. \\n')\n",
    "\n",
    "        # save obj\n",
    "        \n",
    "        self.save_obj(self.train_path, offset_sessions, self.ori_file + '_offset_sessions')\n",
    "        self.save_obj(self.train_path, data, self.ori_file + '_data')\n",
    "        self.save_obj(self.train_path, ItemIdPd, self.ori_file + '_ItemIdPd')\n",
    "        \n",
    "        try:\n",
    "            a = 1/0\n",
    "            G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            grouped = data.groupby('SessionId') \n",
    "            with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('saving meta path edgelists. \\n')\n",
    "                \n",
    "            with open(self.train_path + self.ori_file + '_item_item_edge_list.txt', 'w') as f:\n",
    "                for _, g_i in grouped:\n",
    "                    pair_list = g_i.sort_values(by=['Time'])['ItemId'].tolist()\n",
    "                    if len(pair_list) == 1:\n",
    "                        f.write(\"{} {}\\n\".format(str(pair_list[0]),str(pair_list[0])))\n",
    "                    else:\n",
    "                        window_size = 5\n",
    "                        for l in range(len(pair_list)-1):\n",
    "                            for m in range(l-window_size, l+window_size+1):\n",
    "                                if m<0 or m>=len(pair_list): continue\n",
    "                                f.write(\"{} {}\\n\".format(str(pair_list[l]),str(pair_list[m])))\n",
    "                    \n",
    "                        #for i in range(len(pair_list)-1):\n",
    "                            #f.write(\"{} {}\\n\".format(str(pair_list[i]),str(pair_list[i+1])))\n",
    "                f.close()\n",
    "                \n",
    "\n",
    "    def enrich_edges(self):\n",
    "        G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        G_tag_tag = nx.read_edgelist(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', nodetype=int)\n",
    "        G_item_tag = nx.read_edgelist(self.train_path + self.ori_file + '_item_tag_edge_list.txt', nodetype=int)\n",
    "        \n",
    "        data = self.load_obj(self.train_path, self.ori_file + '_data')\n",
    "        \n",
    "        Set_item_id = set(data['ItemId'].tolist())\n",
    "        Set_item_idx = {}\n",
    "        Set_item_idx_reverse = {}\n",
    "        Set_tag_id = set(data['TagId'].tolist())\n",
    "        Set_tag_idx = {}\n",
    "        Set_tag_idx_reverse = {}\n",
    "\n",
    "        for i,j in enumerate(Set_item_id):\n",
    "            Set_item_idx[j] = i\n",
    "            Set_item_idx_reverse[i] = j\n",
    "        for i,j in enumerate(Set_tag_id):\n",
    "            Set_tag_idx[j] = i\n",
    "            Set_tag_idx_reverse[i] = j\n",
    "            \n",
    "        seq_adj_item_item = np.ones((len(Set_item_id), self.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[i] for i in list(G_item_item.neighbors(node_id))] )\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_item_item[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_tag = np.ones((len(Set_tag_id), self.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[i] for i in list(G_tag_tag.neighbors(node_id))] )\n",
    "            \n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_tag_tag[Set_tag_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_item_tag = np.ones((len(Set_item_id), bert.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[int(i.split(\"_\")[0])] for i in list(G_item_tag.neighbors(str(node_id)))] )\n",
    "            if len(neighbors) > bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=False)\n",
    "            elif len(neighbors) < bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=True)\n",
    "            seq_adj_item_tag[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_item = np.ones((len(Set_tag_id), bert.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[int(i)] for i in list(G_item_tag.neighbors(str(node_id)+\"_tag\"))] )\n",
    "            if len(neighbors) > bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=False)\n",
    "            elif len(neighbors) < bert.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, bert.max_degree, replace=True)\n",
    "            seq_adj_tag_item[Set_tag_idx[node_id],:] = neighbors\n",
    "          \n",
    "\n",
    "        \n",
    "            \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('enrich meta path edgelists. \\n')\n",
    "                \n",
    "        print('enrich meta path edgelists')\n",
    "                \n",
    "        with open(self.train_path + self.ori_file + '_item_item_edge_list.txt', 'a') as f:\n",
    "            for item in G_item_item.nodes():\n",
    "                for item_i in range(G_item_item.degree(item)):\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[item]])]\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[random_tag_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(item_i),str(random_item_id)))\n",
    "            f.close()\n",
    "        \n",
    "        with open(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', 'a') as f:\n",
    "            for tag in G_tag_tag.nodes():\n",
    "                for tag_i in range(G_tag_tag.degree(tag)):\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[tag]] )]\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[random_item_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(tag),str(random_tag_id)))\n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "\n",
    "    def loaddata(self, verbose=False):\n",
    "        # load obj\n",
    "        print('loading data by pickle. \\n')\n",
    "\n",
    "        train_path = self.train_path\n",
    "        ori_file = self.ori_file\n",
    "        self.offset_sessions = self.load_obj(train_path, ori_file + '_offset_sessions')\n",
    "        self.data = self.load_obj(train_path, ori_file + '_data')\n",
    "        self.ItemIdPd = self.load_obj(train_path, ori_file + '_ItemIdPd')\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != '']\n",
    "\n",
    "        self.ItemIdPd.dropna(inplace=True)\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != 'nan']\n",
    "\n",
    "        self.ItemIdList = [int(k) for k in self.ItemIdPd.ItemId.tolist()]\n",
    "        self.ItemIdxList = [int(k) for k in self.ItemIdPd.ItemIdx.tolist()]\n",
    "\n",
    "\n",
    "        #test_cut = int(self.data.shape[0] * 1 / 2)\n",
    "        #self.data = self.data.iloc[test_cut:-1]\n",
    "        #self.data_test = self.data.iloc[0:test_cut]\n",
    "        \n",
    "        item_dict={}\n",
    "        for i,j in zip(self.ItemIdList, self.ItemIdxList):\n",
    "            item_dict[i] = j\n",
    "            \n",
    "        self.G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        print(\"start convert item G to tensor\")\n",
    "        self.adj_item_item = np.zeros((len(self.ItemIdList)+2, self.max_degree))\n",
    "        for node_id in self.ItemIdList:\n",
    "            if self.G_item_item.has_node(node_id):\n",
    "                neighbors = np.array( [item_dict[i] for i in list(self.G_item_item.neighbors(node_id))] )\n",
    "\n",
    "                if len(neighbors) > self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "                elif len(neighbors) < self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            else:\n",
    "                neighbors = np.ones((1,self.max_degree)) * (item_dict[node_id])\n",
    "                \n",
    "            self.adj_item_item[item_dict[node_id]+1,:] = neighbors\n",
    "            \n",
    "        self.adj_item_item_tensor = tf.convert_to_tensor(self.adj_item_item, dtype = tf.int32)  \n",
    "        \n",
    "        ### \n",
    "        self.random_item_item = np.zeros((len(self.ItemIdList)+2, self.max_degree))\n",
    "        neighbors = np.array( [item_dict[i] for i in list(self.G_item_item)] )\n",
    "        for node_id in self.ItemIdList:\n",
    "            if self.G_item_item.has_node(node_id):\n",
    "                if len(neighbors) > self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "                elif len(neighbors) < self.max_degree:\n",
    "                    neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            else:\n",
    "                neighbors = np.ones((1,self.max_degree)) * (item_dict[node_id])\n",
    "                \n",
    "            self.random_item_item[item_dict[node_id]+1,:] = neighbors\n",
    "            \n",
    "        self.random_item_item_tensor = tf.convert_to_tensor(self.random_item_item, dtype = tf.int32)  \n",
    "        \n",
    "\n",
    "    def toFloat(self, inVal):\n",
    "        try:\n",
    "            outVal = float(inVal)\n",
    "        except:\n",
    "            outVal = 0.0\n",
    "\n",
    "        if np.isnan(outVal):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return outVal\n",
    "\n",
    "        \n",
    "    def test(self):\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "        \n",
    "        self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        session_idx_arr = np.arange(len(self.offset_sessions) - 1)\n",
    "        \n",
    "        self.session_embedding_matrix = np.zeros([len(self.offset_sessions), self.rnn_size])\n",
    "        self.session_cluster_matrix = np.zeros([len(self.offset_sessions), 1])\n",
    "        \n",
    "        self.session_words_matrix = np.zeros([len(self.offset_sessions), self.rnn_size])\n",
    "        \n",
    "        self.sess_doc_array = []\n",
    "        self.sess_cluster_array = []\n",
    "        grouped = self.data.groupby('SessionId')\n",
    "        for _, g_i in grouped:\n",
    "            self.sess_doc_array.append(g_i['SessionId'].tolist()[0])\n",
    "            self.sess_cluster_array.append(g_i['Cluster'].tolist()[0])\n",
    "            \n",
    "        for s_idx in session_idx_arr:\n",
    "            start = self.offset_sessions[s_idx]\n",
    "            end = self.offset_sessions[s_idx + 1] - 1\n",
    "            if end - start > 1:\n",
    "                pos_idx = end - 1\n",
    "                if self.sequence_length < end - start:\n",
    "                    end = start + self.sequence_length\n",
    "                    pos_idx = end - 1\n",
    "                self.selected_sess = np.arange(start, pos_idx)\n",
    "                self.len_selected_sess = len(self.selected_sess)\n",
    "                \n",
    "                self.in_idx = np.array(self.data.ItemId.values[self.selected_sess].tolist())\n",
    "                self.out_idx = np.array(self.data.ItemId.values[end])\n",
    "                \n",
    "                self.in_time = self.data.Time.values[self.selected_sess].tolist()\n",
    "                self.out_time = self.data.Time.values[self.selected_sess+1].tolist()\n",
    "                \n",
    "                self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.in_time))]\n",
    "                self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                \n",
    "                self.in_idx_matrix = np.zeros([1, self.sequence_length], dtype=int)\n",
    "                self.in_time_diff_matrix = np.zeros([1, self.sequence_length], dtype=int) + 28\n",
    "        \n",
    "                self.in_idx_matrix = np.concatenate([np.reshape(self.in_idx,[1,-1]), self.in_idx_matrix[:, self.len_selected_sess:]],1)\n",
    "                self.in_time_diff_matrix = np.concatenate([np.reshape(self.in_time_diff,[1,-1]), self.in_time_diff_matrix[:, self.len_selected_sess:]],1) \n",
    "                \n",
    "                def flip_swap_zero(matrix_in, fillval=0):\n",
    "                    matrix_2d = []\n",
    "                    for batch_cnt in range(matrix_in.shape[0]):\n",
    "                        tmp_val = matrix_in[batch_cnt]\n",
    "                        tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                        matrix_2d.append(np.concatenate(\n",
    "                            [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                    return matrix_2d\n",
    "                    \n",
    "                \n",
    "                self.in_idx_matrix_swap = flip_swap_zero(self.in_idx_matrix)\n",
    "                \n",
    "                self.in_time_diff_matrix_swap = flip_swap_zero(self.in_time_diff_matrix, fillval=28)\n",
    "                \n",
    "                \n",
    "                feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(self.in_idx_matrix_swap),\n",
    "                                 \n",
    "                                  self.X_time_diff: np.nan_to_num(self.in_time_diff_matrix_swap),\n",
    "\n",
    "                                  #self.target_item_id_tensor: [np.nan_to_num(np.array(self.out_idx))],\n",
    "\n",
    "                                  \n",
    "                                  self.batch_size: 1\n",
    "                                  }\n",
    "                \n",
    "                \n",
    "                fetches_test = [self.user_final, self.inputs_item]\n",
    "                self.batch_user_final, self.user_words_emb = self.sess.run(fetches_test, feed_dict_test)\n",
    "                self.session_embedding_matrix[int(self.sess_doc_array[s_idx]),:] = self.batch_user_final\n",
    "                self.session_cluster_matrix[int(self.sess_doc_array[s_idx])] = self.sess_cluster_array[s_idx]\n",
    "                \n",
    "                #print(self.user_words_emb.shape)\n",
    "                self.session_words_matrix[int(self.sess_doc_array[s_idx])] = np.mean(self.user_words_emb, axis=1)\n",
    "        \n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "\n",
    "        def sigmoid(input_val):\n",
    "            return 1 / (1 + (math.e ** -input_val))\n",
    "\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            \n",
    "\n",
    "            self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "            self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "            cost_train_list = []\n",
    "            cost_test_list = []\n",
    "            cut_cost_test_list = []\n",
    "\n",
    "            auc_train_list = []\n",
    "            auc_test_list = []\n",
    "            cut_auc_test_list = []\n",
    "            \n",
    "\n",
    "            session_idx_arr = np.arange(len(self.offset_sessions) - 1)\n",
    "            iters = np.arange(self.batch_size_pos)\n",
    "            maxiter = iters.max()  # self.batch_size - 1\n",
    "            # iters is a array, so start will return a array with size of batch_size\n",
    "            start = self.offset_sessions[session_idx_arr[iters]]\n",
    "            end = self.offset_sessions[session_idx_arr[iters] + 1]\n",
    "            finished = False\n",
    "            \n",
    "            max_test_auc = 0\n",
    "            tolerant_time = self.tolerant_time\n",
    "\n",
    "            train_global_step = 0\n",
    "            step = 0\n",
    "\n",
    "            in_idx_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int)\n",
    "            #in_tag_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int)\n",
    "            in_time_diff_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int) + 28\n",
    "\n",
    "            while not finished:\n",
    "                minlen = (end - start).min()\n",
    "                out_idx = self.data.ItemId.values[start]\n",
    "                #out_tag = self.data.TagId.values[start]\n",
    "                out_sessionId = self.data.SessionId.values[start]\n",
    "                out_time = self.data.Time.values[start]\n",
    "                \n",
    "                ###\n",
    "                for i in range(minlen - 1):\n",
    "\n",
    "                    pos_idx = start + i\n",
    "                    in_idx = self.data.ItemId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    #in_tag = self.data.TagId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    in_sessionId = self.data.SessionId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "\n",
    "                    train_idx = start + i + 1\n",
    "                    out_idx = self.data.ItemId.values[train_idx]\n",
    "\n",
    "                    # build time diff\n",
    "                    self.in_time = self.data.Time.values[pos_idx].tolist()\n",
    "                    self.out_time = self.data.Time.values[train_idx].tolist()\n",
    "                    \n",
    "                    self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.out_time))]\n",
    "                    self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                    self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                    self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                   \n",
    "\n",
    "                    in_idx_matrix = np.concatenate([np.reshape(np.array(in_idx), [-1, 1]), in_idx_matrix[:, 0:-1]], 1)\n",
    "                    #in_tag_matrix = np.concatenate([np.reshape(np.array(in_tag), [-1, 1]), #in_tag_matrix[:, 0:-1]], 1)\n",
    "                    \n",
    "                    #self.in_idx_matrix = in_idx_matrix\n",
    "                    \n",
    "                    in_time_diff_matrix = np.concatenate([np.reshape(np.array(self.in_time_diff),\n",
    "                                                                     [-1, 1]), in_time_diff_matrix[:, 0:-1]], 1)\n",
    "\n",
    "                    def flip_swap_zero(matrix_in, fillval=0):\n",
    "                        matrix_2d = []\n",
    "                        for batch_cnt in range(matrix_in.shape[0]):\n",
    "                            tmp_val = matrix_in[batch_cnt]\n",
    "                            tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                            matrix_2d.append(np.concatenate(\n",
    "                                [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                        return matrix_2d\n",
    "                    \n",
    "                    #self.test_in_idx_matrix = in_idx_matrix\n",
    "                    #self.test_in_idx = in_idx\n",
    "                    \n",
    "                    \n",
    "                    in_idx_matrix_swap = flip_swap_zero(in_idx_matrix)\n",
    "                    #in_tag_matrix_swap = flip_swap_zero(in_tag_matrix)\n",
    "                    \n",
    "                    in_time_diff_matrix_swap = flip_swap_zero(in_time_diff_matrix, fillval=28)\n",
    "\n",
    "                   \n",
    "                    \n",
    "                    num_to_select = int(len(in_sessionId) * 19.0 / 20)\n",
    "                    \n",
    "                    \n",
    "                    train_set_index = list(np.arange(num_to_select))\n",
    "                    batch_size_train = len(train_set_index)\n",
    "                    \n",
    "                    \n",
    "                    test_set_index = list(np.arange(num_to_select, len(in_sessionId)))\n",
    "                    batch_size_test = len(test_set_index)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    self.item_tmp = np.nan_to_num(in_idx_matrix_swap)[train_set_index]\n",
    "                    #self.tag_tmp = np.nan_to_num(in_tag_matrix_swap)[train_set_index]\n",
    "                    \n",
    "                    feed_dict = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[train_set_index], self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[train_set_index], self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[train_set_index], self.batch_size: batch_size_train}\n",
    "                    fetches = [self.yhat, self.cost, self.final_state, self.global_step, self.lr, self.train_op]\n",
    "                    #                     fetches = [self.trfm_out]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    train_global_step += 1\n",
    "                    yhat, cost, state, step, lr, _ = self.sess.run(fetches, feed_dict)\n",
    "                    #                     self.trfm_out_check = self.sess.run(fetches, feed_dict)\n",
    "                    #                     print(self.trfm_out_check)\n",
    "\n",
    "                    #                     return\n",
    "                    \n",
    "\n",
    "                    # evaluate training result\n",
    "                    fetches = [self.yhat, self.cost]\n",
    "                    yhat_train, cost_train = self.sess.run(fetches, feed_dict)\n",
    "\n",
    "\n",
    "                    \n",
    "                    feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[test_set_index],self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[test_set_index], self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[test_set_index], self.batch_size: batch_size_test}\n",
    "\n",
    "                    fetches_test = [self.yhat, self.cost]\n",
    "                    yhat_test, cost_test = self.sess.run(fetches_test, feed_dict_test)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    if np.isnan(cost):\n",
    "                        print(str(epoch) + ':Nan error!')\n",
    "                        self.error_during_train = True\n",
    "                        return\n",
    "\n",
    "                    cost_train_list.append(cost_train)\n",
    "\n",
    "                    label_train = np.eye(yhat_train.shape[0])\n",
    "                    self.yhat_train = yhat_train\n",
    "                    self.label_train = label_train\n",
    "                    label_1d_train = np.reshape(label_train, [-1])\n",
    "                    prob_1d_train = np.reshape(yhat_train, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_train, prob_1d_train, pos_label=1)\n",
    "                    auc_train = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    label_test = np.eye(yhat_test.shape[0])\n",
    "                    label_1d_test = np.reshape(label_test, [-1])\n",
    "                    prob_1d_test = np.reshape(yhat_test, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_test, prob_1d_test, pos_label=1)\n",
    "                    auc_test = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    auc_train_list.append(auc_train)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if step % 20 == 0:\n",
    "                        #cost_test, auc_test = self.test_data()\n",
    "                        \n",
    "                        auc_test_list.append(auc_test)\n",
    "                        cost_test_list.append(cost_test)\n",
    "                        \n",
    "                        avgc = np.mean(cost_train_list)\n",
    "                        avgc_test = np.mean(cost_test_list)\n",
    "                        auc_train_avg = np.mean(auc_train_list)\n",
    "                        auc_test_avg = np.mean(auc_test_list)\n",
    "\n",
    "                        print('{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "\n",
    "                        if step > 10:\n",
    "                            cost_train_list = []\n",
    "                            cost_test_list = []\n",
    "                            auc_train_list = []\n",
    "                            auc_test_list = []\n",
    "                \n",
    "                        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                            log_file.write( '{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "                            \n",
    "                            if max_test_auc > auc_test_avg:\n",
    "                                if epoch > 0:\n",
    "                                    tolerant_time -= 1\n",
    "                                if tolerant_time == 0:\n",
    "                                    finished = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                max_test_auc = auc_test_avg\n",
    "                                tolerant_time = self.tolerant_time\n",
    "                                self.saver.save(self.sess, '{}/bert-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "\n",
    "                start = start + minlen - 1\n",
    "                mask = np.arange(len(iters))[(end - start) <= 1]  # idx of ended sessions\n",
    "                for idx in mask:\n",
    "                    maxiter += 1\n",
    "                    if maxiter >= len(self.offset_sessions) - 1:\n",
    "                        finished = True\n",
    "                        break\n",
    "                    iters[idx] = maxiter\n",
    "                    start[idx] = self.offset_sessions[session_idx_arr[maxiter]]\n",
    "                    end[idx] = self.offset_sessions[session_idx_arr[maxiter] + 1]\n",
    "\n",
    "                if len(mask) and self.reset_after_session:\n",
    "                    in_idx_matrix[mask, :] = 0\n",
    "                    #in_tag_matrix[mask, :] = 0\n",
    "                  \n",
    "                    in_time_diff_matrix[mask, :] = 28\n",
    "\n",
    "            if np.isnan(avgc):\n",
    "                print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "                with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                    log_file.write( '{} Epoch {}: Nan error!'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), epoch,\n",
    "                                                         avgc) + \"\\n\")\n",
    "                self.error_during_train = True\n",
    "                return\n",
    "            self.saver.save(self.sess, '{}/bert-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "            with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('ckpt to : ' + self.checkpoint_dir)\n",
    "        '''        \n",
    "        cost_test, auc_test = self.test_data()\n",
    "        cut_auc_test_list.append(auc_test)\n",
    "        cut_cost_test_list.append(cost_test)\n",
    "        cut_avgc = np.mean(cut_cost_test_list)\n",
    "        cut_avgauc = np.mean(cut_auc_test_list)\n",
    "        print('Ori_file{}, Epoch{},cutLossTst:{:.5f},cutAucTst:{:.5f}\\n'.format(\n",
    "                                    self.ori_file, epoch, cut_avgc, cut_avgauc))\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del bert\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used\n",
    "\n",
    "#with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "args.neg_pos_sam_rat = 7\n",
    "\n",
    "#bert.loaddata()\n",
    "#bert.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data by pickle. \n",
      "\n",
      "start convert item G to tensor\n"
     ]
    }
   ],
   "source": [
    "bert = BERT4Rec(sess, args)\n",
    "#bert.preprocess()\n",
    "bert.loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init1\n",
      "init2\n",
      "init3\n"
     ]
    }
   ],
   "source": [
    "bert.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 21:51:32 Ori_fileTweet_process, Epoch0,Step20,lr:0.001000,loss:5.89462,auc:0.50394,lossTst:3.04514,aucTst:0.46939,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:34 Ori_fileTweet_process, Epoch0,Step40,lr:0.001000,loss:5.89360,auc:0.51168,lossTst:3.03314,aucTst:0.65306,MaxaucTst:0.46939,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:35 Ori_fileTweet_process, Epoch0,Step60,lr:0.001000,loss:5.86954,auc:0.56473,lossTst:3.00122,aucTst:0.66667,MaxaucTst:0.65306,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:37 Ori_fileTweet_process, Epoch0,Step80,lr:0.001000,loss:5.70579,auc:0.65276,lossTst:2.68190,aucTst:0.74490,MaxaucTst:0.66667,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:38 Ori_fileTweet_process, Epoch0,Step100,lr:0.001000,loss:5.57596,auc:0.70097,lossTst:2.57071,aucTst:0.70408,MaxaucTst:0.74490,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:40 Ori_fileTweet_process, Epoch0,Step120,lr:0.001000,loss:5.52863,auc:0.71453,lossTst:2.19532,aucTst:0.86735,MaxaucTst:0.74490,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:41 Ori_fileTweet_process, Epoch0,Step140,lr:0.001000,loss:5.43116,auc:0.74594,lossTst:3.11286,aucTst:0.53401,MaxaucTst:0.86735,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:43 Ori_fileTweet_process, Epoch1,Step160,lr:0.001000,loss:5.40065,auc:0.75067,lossTst:2.25338,aucTst:0.88095,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:44 Ori_fileTweet_process, Epoch1,Step180,lr:0.001000,loss:5.32281,auc:0.77763,lossTst:2.50838,aucTst:0.75850,MaxaucTst:0.88095,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:45 Ori_fileTweet_process, Epoch1,Step200,lr:0.001000,loss:5.23969,auc:0.79468,lossTst:2.64165,aucTst:0.75510,MaxaucTst:0.88095,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:51:47 Ori_fileTweet_process, Epoch1,Step220,lr:0.001000,loss:5.28182,auc:0.78249,lossTst:2.35492,aucTst:0.87415,MaxaucTst:0.88095,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:51:48 Ori_fileTweet_process, Epoch1,Step240,lr:0.001000,loss:5.11796,auc:0.82167,lossTst:2.30034,aucTst:0.84354,MaxaucTst:0.88095,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:51:49 Ori_fileTweet_process, Epoch1,Step260,lr:0.001000,loss:5.11836,auc:0.82046,lossTst:2.17730,aucTst:0.84354,MaxaucTst:0.88095,tolerant:36.0\n",
      "\n",
      "2022-04-14 21:51:50 Ori_fileTweet_process, Epoch1,Step280,lr:0.001000,loss:4.98020,auc:0.84508,lossTst:3.39182,aucTst:0.47959,MaxaucTst:0.88095,tolerant:35.0\n",
      "\n",
      "2022-04-14 21:51:51 Ori_fileTweet_process, Epoch2,Step300,lr:0.001000,loss:4.94242,auc:0.84664,lossTst:2.28587,aucTst:0.84354,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:53 Ori_fileTweet_process, Epoch2,Step320,lr:0.001000,loss:4.81667,auc:0.86633,lossTst:2.37722,aucTst:0.81293,MaxaucTst:0.84354,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:54 Ori_fileTweet_process, Epoch2,Step340,lr:0.001000,loss:4.72862,auc:0.87551,lossTst:2.67477,aucTst:0.70408,MaxaucTst:0.84354,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:51:55 Ori_fileTweet_process, Epoch2,Step360,lr:0.001000,loss:4.76280,auc:0.86924,lossTst:2.43554,aucTst:0.83333,MaxaucTst:0.84354,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:51:56 Ori_fileTweet_process, Epoch2,Step380,lr:0.001000,loss:4.64605,auc:0.88478,lossTst:1.47087,aucTst:0.96259,MaxaucTst:0.84354,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:51:58 Ori_fileTweet_process, Epoch2,Step400,lr:0.001000,loss:4.59426,auc:0.89018,lossTst:2.39932,aucTst:0.85034,MaxaucTst:0.96259,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:51:59 Ori_fileTweet_process, Epoch2,Step420,lr:0.001000,loss:4.47029,auc:0.90215,lossTst:2.54472,aucTst:0.76871,MaxaucTst:0.96259,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:00 Ori_fileTweet_process, Epoch3,Step440,lr:0.001000,loss:4.42680,auc:0.90443,lossTst:2.66242,aucTst:0.74830,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:02 Ori_fileTweet_process, Epoch3,Step460,lr:0.001000,loss:4.34694,auc:0.91102,lossTst:2.55899,aucTst:0.79932,MaxaucTst:0.74830,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:03 Ori_fileTweet_process, Epoch3,Step480,lr:0.001000,loss:4.22716,auc:0.91985,lossTst:2.23404,aucTst:0.86054,MaxaucTst:0.79932,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:05 Ori_fileTweet_process, Epoch3,Step500,lr:0.001000,loss:4.22499,auc:0.91885,lossTst:1.53152,aucTst:0.95578,MaxaucTst:0.86054,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:06 Ori_fileTweet_process, Epoch3,Step520,lr:0.001000,loss:4.18272,auc:0.92172,lossTst:4.61143,aucTst:0.52721,MaxaucTst:0.95578,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:07 Ori_fileTweet_process, Epoch3,Step540,lr:0.001000,loss:4.06907,auc:0.93082,lossTst:2.42060,aucTst:0.83673,MaxaucTst:0.95578,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:09 Ori_fileTweet_process, Epoch3,Step560,lr:0.001000,loss:4.05837,auc:0.93118,lossTst:2.35406,aucTst:0.83673,MaxaucTst:0.95578,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:52:10 Ori_fileTweet_process, Epoch4,Step580,lr:0.001000,loss:4.01667,auc:0.93166,lossTst:2.47280,aucTst:0.81293,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:12 Ori_fileTweet_process, Epoch4,Step600,lr:0.001000,loss:3.89028,auc:0.94009,lossTst:1.54920,aucTst:0.95578,MaxaucTst:0.81293,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:13 Ori_fileTweet_process, Epoch4,Step620,lr:0.001000,loss:3.78858,auc:0.94391,lossTst:3.22617,aucTst:0.77211,MaxaucTst:0.95578,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:14 Ori_fileTweet_process, Epoch4,Step640,lr:0.001000,loss:3.81147,auc:0.94245,lossTst:2.43107,aucTst:0.86735,MaxaucTst:0.95578,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:15 Ori_fileTweet_process, Epoch4,Step660,lr:0.001000,loss:3.81080,auc:0.94287,lossTst:1.80984,aucTst:0.91837,MaxaucTst:0.95578,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:52:16 Ori_fileTweet_process, Epoch4,Step680,lr:0.001000,loss:3.71866,auc:0.94926,lossTst:2.68397,aucTst:0.82313,MaxaucTst:0.95578,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:52:18 Ori_fileTweet_process, Epoch4,Step700,lr:0.001000,loss:3.72952,auc:0.94706,lossTst:3.28692,aucTst:0.69728,MaxaucTst:0.95578,tolerant:36.0\n",
      "\n",
      "2022-04-14 21:52:19 Ori_fileTweet_process, Epoch5,Step720,lr:0.001000,loss:3.72698,auc:0.94698,lossTst:1.84802,aucTst:0.91156,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:21 Ori_fileTweet_process, Epoch5,Step740,lr:0.001000,loss:3.55399,auc:0.95543,lossTst:2.49025,aucTst:0.78571,MaxaucTst:0.91156,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:22 Ori_fileTweet_process, Epoch5,Step760,lr:0.001000,loss:3.49362,auc:0.95753,lossTst:3.01401,aucTst:0.77211,MaxaucTst:0.91156,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:23 Ori_fileTweet_process, Epoch5,Step780,lr:0.001000,loss:3.49494,auc:0.95656,lossTst:3.81842,aucTst:0.71088,MaxaucTst:0.91156,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:52:24 Ori_fileTweet_process, Epoch5,Step800,lr:0.001000,loss:3.54107,auc:0.95551,lossTst:2.83073,aucTst:0.80952,MaxaucTst:0.91156,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:52:25 Ori_fileTweet_process, Epoch5,Step820,lr:0.001000,loss:3.40934,auc:0.96179,lossTst:1.22546,aucTst:0.98299,MaxaucTst:0.91156,tolerant:36.0\n",
      "\n",
      "2022-04-14 21:52:26 Ori_fileTweet_process, Epoch5,Step840,lr:0.001000,loss:3.40320,auc:0.96073,lossTst:3.03078,aucTst:0.80272,MaxaucTst:0.98299,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:28 Ori_fileTweet_process, Epoch6,Step860,lr:0.001000,loss:3.45872,auc:0.95956,lossTst:1.98894,aucTst:0.90476,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:29 Ori_fileTweet_process, Epoch6,Step880,lr:0.001000,loss:3.35550,auc:0.96256,lossTst:5.07239,aucTst:0.52721,MaxaucTst:0.90476,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:31 Ori_fileTweet_process, Epoch6,Step900,lr:0.001000,loss:3.21724,auc:0.96731,lossTst:2.09574,aucTst:0.90816,MaxaucTst:0.90476,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:32 Ori_fileTweet_process, Epoch6,Step920,lr:0.001000,loss:3.21546,auc:0.96769,lossTst:3.73560,aucTst:0.74830,MaxaucTst:0.90816,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:34 Ori_fileTweet_process, Epoch6,Step940,lr:0.001000,loss:3.29763,auc:0.96421,lossTst:2.49980,aucTst:0.85714,MaxaucTst:0.90816,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:35 Ori_fileTweet_process, Epoch6,Step960,lr:0.001000,loss:3.16385,auc:0.97027,lossTst:2.94144,aucTst:0.82993,MaxaucTst:0.90816,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:52:36 Ori_fileTweet_process, Epoch6,Step980,lr:0.001000,loss:3.14989,auc:0.96946,lossTst:2.81835,aucTst:0.81633,MaxaucTst:0.90816,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:52:38 Ori_fileTweet_process, Epoch7,Step1000,lr:0.001000,loss:3.20925,auc:0.96863,lossTst:2.28145,aucTst:0.89456,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:40 Ori_fileTweet_process, Epoch7,Step1020,lr:0.000960,loss:3.13747,auc:0.97036,lossTst:2.38601,aucTst:0.89116,MaxaucTst:0.89456,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:41 Ori_fileTweet_process, Epoch7,Step1040,lr:0.000960,loss:3.03895,auc:0.97248,lossTst:0.80921,aucTst:1.00000,MaxaucTst:0.89456,tolerant:39.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 21:52:43 Ori_fileTweet_process, Epoch7,Step1060,lr:0.000960,loss:2.99352,auc:0.97499,lossTst:3.34925,aucTst:0.84694,MaxaucTst:1.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:44 Ori_fileTweet_process, Epoch7,Step1080,lr:0.000960,loss:2.98420,auc:0.97293,lossTst:0.74646,aucTst:1.00000,MaxaucTst:1.00000,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:45 Ori_fileTweet_process, Epoch7,Step1100,lr:0.000960,loss:2.90197,auc:0.97757,lossTst:3.21305,aucTst:0.79932,MaxaucTst:1.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:47 Ori_fileTweet_process, Epoch7,Step1120,lr:0.000960,loss:2.96223,auc:0.97479,lossTst:0.97010,aucTst:0.99660,MaxaucTst:1.00000,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:48 Ori_fileTweet_process, Epoch8,Step1140,lr:0.000960,loss:2.98998,auc:0.97684,lossTst:2.15491,aucTst:0.88435,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:49 Ori_fileTweet_process, Epoch8,Step1160,lr:0.000960,loss:2.92195,auc:0.97642,lossTst:3.22418,aucTst:0.80952,MaxaucTst:0.88435,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:51 Ori_fileTweet_process, Epoch8,Step1180,lr:0.000960,loss:2.83656,auc:0.97742,lossTst:1.45384,aucTst:0.97279,MaxaucTst:0.88435,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:52 Ori_fileTweet_process, Epoch8,Step1200,lr:0.000960,loss:2.77605,auc:0.98014,lossTst:1.96542,aucTst:0.91156,MaxaucTst:0.97279,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:53 Ori_fileTweet_process, Epoch8,Step1220,lr:0.000960,loss:2.76461,auc:0.97846,lossTst:1.75517,aucTst:0.93197,MaxaucTst:0.97279,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:52:54 Ori_fileTweet_process, Epoch8,Step1240,lr:0.000960,loss:2.69246,auc:0.98175,lossTst:2.99362,aucTst:0.81633,MaxaucTst:0.97279,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:52:55 Ori_fileTweet_process, Epoch8,Step1260,lr:0.000960,loss:2.78057,auc:0.97951,lossTst:1.74079,aucTst:0.93537,MaxaucTst:0.97279,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:52:57 Ori_fileTweet_process, Epoch9,Step1280,lr:0.000960,loss:2.80363,auc:0.98011,lossTst:2.40194,aucTst:0.89456,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:52:58 Ori_fileTweet_process, Epoch9,Step1300,lr:0.000960,loss:2.72014,auc:0.98131,lossTst:1.92558,aucTst:0.90816,MaxaucTst:0.89456,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:53:00 Ori_fileTweet_process, Epoch9,Step1320,lr:0.000960,loss:2.62375,auc:0.98271,lossTst:3.41561,aucTst:0.78571,MaxaucTst:0.90816,tolerant:40.0\n",
      "\n",
      "2022-04-14 21:53:01 Ori_fileTweet_process, Epoch9,Step1340,lr:0.000960,loss:2.60611,auc:0.98341,lossTst:3.89670,aucTst:0.72109,MaxaucTst:0.90816,tolerant:39.0\n",
      "\n",
      "2022-04-14 21:53:02 Ori_fileTweet_process, Epoch9,Step1360,lr:0.000960,loss:2.61266,auc:0.98229,lossTst:3.60853,aucTst:0.72449,MaxaucTst:0.90816,tolerant:38.0\n",
      "\n",
      "2022-04-14 21:53:03 Ori_fileTweet_process, Epoch9,Step1380,lr:0.000960,loss:2.58049,auc:0.98430,lossTst:6.02284,aucTst:0.64286,MaxaucTst:0.90816,tolerant:37.0\n",
      "\n",
      "2022-04-14 21:53:04 Ori_fileTweet_process, Epoch9,Step1400,lr:0.000960,loss:2.54844,auc:0.98365,lossTst:0.94347,aucTst:0.99660,MaxaucTst:0.90816,tolerant:36.0\n",
      "\n",
      "2022-04-14 21:53:06 Ori_fileTweet_process, Epoch9,Step1420,lr:0.000960,loss:2.54154,auc:0.98440,lossTst:5.45508,aucTst:0.70068,MaxaucTst:0.99660,tolerant:40.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert.n_epochs = 10\n",
    "bert.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [str(int(float(i[0]))) for i in list(bert.session_cluster_matrix)]\n",
    "X = [str(i) for i in list(np.arange(len(Y)))]\n",
    "Y = Y[:-1]\n",
    "X = X[:-1]\n",
    "doc_emb = bert.session_embedding_matrix[:-1]\n",
    "\n",
    "#doc_emb = normalize(doc_emb, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2472, 256)"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_label = [int(float(i)) for i in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.6558789289871944\n",
      "0.2   0.6881889763779527\n",
      "0.3   0.7116160861115263\n",
      "0.4   0.7177250523377529\n",
      "0.5   0.7305916911456147\n",
      "0.6   0.7321802935010483\n",
      "0.7   0.742997198879552\n",
      "0.8   0.7518401682439537\n",
      "0.9   0.75\n",
      "0.720113155064955\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['micro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['micro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.6275265007154734\n",
      "0.2   0.6644526295890187\n",
      "0.3   0.6853755605148616\n",
      "0.4   0.6933497671485498\n",
      "0.5   0.7088335858371684\n",
      "0.6   0.7103223299253463\n",
      "0.7   0.7194302677979465\n",
      "0.8   0.7282161986687872\n",
      "0.9   0.7244472573889214\n",
      "0.6957726775095636\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['macro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['macro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.3707865168539326\n",
      "0.2   0.4115267947421638\n",
      "0.3   0.4482957827845176\n",
      "0.4   0.44609164420485176\n",
      "0.5   0.46763754045307443\n",
      "0.6   0.46309403437815977\n",
      "0.7   0.46765498652291104\n",
      "0.8   0.4767676767676768\n",
      "0.9   0.4596774193548387\n",
      "0.4457258217846808\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
